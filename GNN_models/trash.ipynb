{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.63994, Train Acc: 0.67166, Test Acc: 0.77477\n",
      "Epoch: 002, Loss: 0.60602, Train Acc: 0.72555, Test Acc: 0.75676\n",
      "Epoch: 003, Loss: 0.59269, Train Acc: 0.73553, Test Acc: 0.78378\n",
      "Epoch: 004, Loss: 0.57202, Train Acc: 0.73353, Test Acc: 0.75676\n",
      "Epoch: 005, Loss: 0.56646, Train Acc: 0.73553, Test Acc: 0.72973\n",
      "Epoch: 006, Loss: 0.56377, Train Acc: 0.74451, Test Acc: 0.76577\n",
      "Epoch: 007, Loss: 0.53971, Train Acc: 0.74950, Test Acc: 0.77477\n",
      "Epoch: 008, Loss: 0.54838, Train Acc: 0.74750, Test Acc: 0.76577\n",
      "Epoch: 009, Loss: 0.54948, Train Acc: 0.75349, Test Acc: 0.75676\n",
      "Epoch: 010, Loss: 0.53813, Train Acc: 0.75349, Test Acc: 0.76577\n",
      "Epoch: 011, Loss: 0.53843, Train Acc: 0.75150, Test Acc: 0.76577\n",
      "Epoch: 012, Loss: 0.53526, Train Acc: 0.75649, Test Acc: 0.74775\n",
      "Epoch: 013, Loss: 0.53497, Train Acc: 0.76347, Test Acc: 0.76577\n",
      "Epoch: 014, Loss: 0.52893, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 015, Loss: 0.52412, Train Acc: 0.75848, Test Acc: 0.74775\n",
      "Epoch: 016, Loss: 0.52143, Train Acc: 0.75749, Test Acc: 0.73874\n",
      "Epoch: 017, Loss: 0.51217, Train Acc: 0.76248, Test Acc: 0.76577\n",
      "Epoch: 018, Loss: 0.50869, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 019, Loss: 0.50850, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 020, Loss: 0.50986, Train Acc: 0.76946, Test Acc: 0.73874\n",
      "Epoch: 021, Loss: 0.50446, Train Acc: 0.76647, Test Acc: 0.74775\n",
      "Epoch: 022, Loss: 0.50306, Train Acc: 0.77445, Test Acc: 0.74775\n",
      "Epoch: 023, Loss: 0.50349, Train Acc: 0.77046, Test Acc: 0.73874\n",
      "Epoch: 024, Loss: 0.50693, Train Acc: 0.77246, Test Acc: 0.75676\n",
      "Epoch: 025, Loss: 0.50562, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 026, Loss: 0.50851, Train Acc: 0.77645, Test Acc: 0.75676\n",
      "Epoch: 027, Loss: 0.51166, Train Acc: 0.77445, Test Acc: 0.75676\n",
      "Epoch: 028, Loss: 0.50159, Train Acc: 0.77944, Test Acc: 0.75676\n",
      "Epoch: 029, Loss: 0.49367, Train Acc: 0.78044, Test Acc: 0.74775\n",
      "Epoch: 030, Loss: 0.49450, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 031, Loss: 0.48509, Train Acc: 0.78842, Test Acc: 0.77477\n",
      "Epoch: 032, Loss: 0.49752, Train Acc: 0.78842, Test Acc: 0.75676\n",
      "Epoch: 033, Loss: 0.47788, Train Acc: 0.79242, Test Acc: 0.75676\n",
      "Epoch: 034, Loss: 0.47482, Train Acc: 0.78842, Test Acc: 0.74775\n",
      "Epoch: 035, Loss: 0.48345, Train Acc: 0.79042, Test Acc: 0.75676\n",
      "Epoch: 036, Loss: 0.47648, Train Acc: 0.78643, Test Acc: 0.76577\n",
      "Epoch: 037, Loss: 0.47779, Train Acc: 0.79341, Test Acc: 0.77477\n",
      "Epoch: 038, Loss: 0.47345, Train Acc: 0.79042, Test Acc: 0.77477\n",
      "Epoch: 039, Loss: 0.47719, Train Acc: 0.79142, Test Acc: 0.75676\n",
      "Epoch: 040, Loss: 0.46943, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 041, Loss: 0.45909, Train Acc: 0.79242, Test Acc: 0.79279\n",
      "Epoch: 042, Loss: 0.45949, Train Acc: 0.78942, Test Acc: 0.75676\n",
      "Epoch: 043, Loss: 0.47938, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 044, Loss: 0.45838, Train Acc: 0.80040, Test Acc: 0.77477\n",
      "Epoch: 045, Loss: 0.44669, Train Acc: 0.80140, Test Acc: 0.78378\n",
      "Epoch: 046, Loss: 0.45047, Train Acc: 0.79840, Test Acc: 0.78378\n",
      "Epoch: 047, Loss: 0.46048, Train Acc: 0.80040, Test Acc: 0.75676\n",
      "Epoch: 048, Loss: 0.45371, Train Acc: 0.79840, Test Acc: 0.75676\n",
      "Epoch: 049, Loss: 0.45315, Train Acc: 0.79341, Test Acc: 0.75676\n",
      "Epoch: 050, Loss: 0.44780, Train Acc: 0.79341, Test Acc: 0.76577\n",
      "Epoch: 051, Loss: 0.44573, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 052, Loss: 0.43719, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 053, Loss: 0.45039, Train Acc: 0.80838, Test Acc: 0.76577\n",
      "Epoch: 054, Loss: 0.44865, Train Acc: 0.80339, Test Acc: 0.78378\n",
      "Epoch: 055, Loss: 0.43105, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 056, Loss: 0.43284, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 057, Loss: 0.42553, Train Acc: 0.81238, Test Acc: 0.77477\n",
      "Epoch: 058, Loss: 0.42388, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 059, Loss: 0.42252, Train Acc: 0.80240, Test Acc: 0.74775\n",
      "Epoch: 060, Loss: 0.42339, Train Acc: 0.81337, Test Acc: 0.76577\n",
      "Epoch: 061, Loss: 0.41549, Train Acc: 0.81138, Test Acc: 0.76577\n",
      "Epoch: 062, Loss: 0.41829, Train Acc: 0.81038, Test Acc: 0.75676\n",
      "Epoch: 063, Loss: 0.41101, Train Acc: 0.81038, Test Acc: 0.74775\n",
      "Epoch: 064, Loss: 0.41326, Train Acc: 0.81537, Test Acc: 0.73874\n",
      "Epoch: 065, Loss: 0.40395, Train Acc: 0.80439, Test Acc: 0.75676\n",
      "Epoch: 066, Loss: 0.40527, Train Acc: 0.81936, Test Acc: 0.76577\n",
      "Epoch: 067, Loss: 0.40050, Train Acc: 0.82435, Test Acc: 0.76577\n",
      "Epoch: 068, Loss: 0.39777, Train Acc: 0.82036, Test Acc: 0.76577\n",
      "Epoch: 069, Loss: 0.39965, Train Acc: 0.82635, Test Acc: 0.73874\n",
      "Epoch: 070, Loss: 0.38398, Train Acc: 0.83034, Test Acc: 0.73874\n",
      "Epoch: 071, Loss: 0.39805, Train Acc: 0.82236, Test Acc: 0.74775\n",
      "Epoch: 072, Loss: 0.39025, Train Acc: 0.81836, Test Acc: 0.75676\n",
      "Epoch: 073, Loss: 0.41702, Train Acc: 0.80439, Test Acc: 0.77477\n",
      "Epoch: 074, Loss: 0.41968, Train Acc: 0.81537, Test Acc: 0.76577\n",
      "Epoch: 075, Loss: 0.39959, Train Acc: 0.82735, Test Acc: 0.76577\n",
      "Epoch: 076, Loss: 0.37986, Train Acc: 0.82735, Test Acc: 0.73874\n",
      "Epoch: 077, Loss: 0.38195, Train Acc: 0.83433, Test Acc: 0.76577\n",
      "Epoch: 078, Loss: 0.36943, Train Acc: 0.83733, Test Acc: 0.74775\n",
      "Epoch: 079, Loss: 0.35535, Train Acc: 0.82535, Test Acc: 0.72973\n",
      "Epoch: 080, Loss: 0.37084, Train Acc: 0.82136, Test Acc: 0.72072\n",
      "Epoch: 081, Loss: 0.37048, Train Acc: 0.83034, Test Acc: 0.72973\n",
      "Epoch: 082, Loss: 0.36895, Train Acc: 0.82735, Test Acc: 0.69369\n",
      "Epoch: 083, Loss: 0.37141, Train Acc: 0.83533, Test Acc: 0.76577\n",
      "Epoch: 084, Loss: 0.35657, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 085, Loss: 0.35072, Train Acc: 0.82934, Test Acc: 0.70270\n",
      "Epoch: 086, Loss: 0.35426, Train Acc: 0.84331, Test Acc: 0.72973\n",
      "Epoch: 087, Loss: 0.34678, Train Acc: 0.85130, Test Acc: 0.70270\n",
      "Epoch: 088, Loss: 0.33941, Train Acc: 0.84531, Test Acc: 0.72973\n",
      "Epoch: 089, Loss: 0.33547, Train Acc: 0.85329, Test Acc: 0.73874\n",
      "Epoch: 090, Loss: 0.32926, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 091, Loss: 0.31867, Train Acc: 0.86327, Test Acc: 0.73874\n",
      "Epoch: 092, Loss: 0.32957, Train Acc: 0.86128, Test Acc: 0.70270\n",
      "Epoch: 093, Loss: 0.31665, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 094, Loss: 0.31365, Train Acc: 0.85429, Test Acc: 0.76577\n",
      "Epoch: 095, Loss: 0.32666, Train Acc: 0.85928, Test Acc: 0.74775\n",
      "Epoch: 096, Loss: 0.32321, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 097, Loss: 0.32056, Train Acc: 0.87425, Test Acc: 0.77477\n",
      "Epoch: 098, Loss: 0.30211, Train Acc: 0.86627, Test Acc: 0.73874\n",
      "Epoch: 099, Loss: 0.31116, Train Acc: 0.85928, Test Acc: 0.78378\n",
      "Epoch: 100, Loss: 0.30914, Train Acc: 0.87625, Test Acc: 0.76577\n",
      "Epoch: 101, Loss: 0.31156, Train Acc: 0.87625, Test Acc: 0.75676\n",
      "Epoch: 102, Loss: 0.29537, Train Acc: 0.87824, Test Acc: 0.72072\n",
      "Epoch: 103, Loss: 0.30042, Train Acc: 0.88723, Test Acc: 0.74775\n",
      "Epoch: 104, Loss: 0.28999, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 105, Loss: 0.28154, Train Acc: 0.86427, Test Acc: 0.74775\n",
      "Epoch: 106, Loss: 0.27667, Train Acc: 0.88523, Test Acc: 0.77477\n",
      "Epoch: 107, Loss: 0.26873, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 108, Loss: 0.28136, Train Acc: 0.89022, Test Acc: 0.77477\n",
      "Epoch: 109, Loss: 0.27982, Train Acc: 0.89421, Test Acc: 0.76577\n",
      "Epoch: 110, Loss: 0.26023, Train Acc: 0.90519, Test Acc: 0.72072\n",
      "Epoch: 111, Loss: 0.26328, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 112, Loss: 0.25424, Train Acc: 0.89621, Test Acc: 0.73874\n",
      "Epoch: 113, Loss: 0.25966, Train Acc: 0.89421, Test Acc: 0.78378\n",
      "Epoch: 114, Loss: 0.24187, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 115, Loss: 0.25082, Train Acc: 0.89820, Test Acc: 0.77477\n",
      "Epoch: 116, Loss: 0.28134, Train Acc: 0.91317, Test Acc: 0.73874\n",
      "Epoch: 117, Loss: 0.24313, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 118, Loss: 0.25145, Train Acc: 0.90918, Test Acc: 0.72973\n",
      "Epoch: 119, Loss: 0.26201, Train Acc: 0.90319, Test Acc: 0.72973\n",
      "Epoch: 120, Loss: 0.23378, Train Acc: 0.91617, Test Acc: 0.69369\n",
      "Epoch: 121, Loss: 0.24473, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 122, Loss: 0.23172, Train Acc: 0.91517, Test Acc: 0.75676\n",
      "Epoch: 123, Loss: 0.23335, Train Acc: 0.92016, Test Acc: 0.75676\n",
      "Epoch: 124, Loss: 0.20975, Train Acc: 0.92216, Test Acc: 0.73874\n",
      "Epoch: 125, Loss: 0.20874, Train Acc: 0.92515, Test Acc: 0.76577\n",
      "Epoch: 126, Loss: 0.20907, Train Acc: 0.93014, Test Acc: 0.77477\n",
      "Epoch: 127, Loss: 0.20823, Train Acc: 0.92515, Test Acc: 0.70270\n",
      "Epoch: 128, Loss: 0.20784, Train Acc: 0.93513, Test Acc: 0.73874\n",
      "Epoch: 129, Loss: 0.20660, Train Acc: 0.93812, Test Acc: 0.74775\n",
      "Epoch: 130, Loss: 0.22070, Train Acc: 0.92914, Test Acc: 0.71171\n",
      "Epoch: 131, Loss: 0.22617, Train Acc: 0.93413, Test Acc: 0.78378\n",
      "Epoch: 132, Loss: 0.20191, Train Acc: 0.93513, Test Acc: 0.71171\n",
      "Epoch: 133, Loss: 0.20896, Train Acc: 0.93613, Test Acc: 0.76577\n",
      "Epoch: 134, Loss: 0.22466, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 135, Loss: 0.24093, Train Acc: 0.93613, Test Acc: 0.66667\n",
      "Epoch: 136, Loss: 0.21365, Train Acc: 0.93812, Test Acc: 0.70270\n",
      "Epoch: 137, Loss: 0.22885, Train Acc: 0.92016, Test Acc: 0.73874\n",
      "Epoch: 138, Loss: 0.23151, Train Acc: 0.92016, Test Acc: 0.69369\n",
      "Epoch: 139, Loss: 0.23535, Train Acc: 0.92715, Test Acc: 0.72973\n",
      "Epoch: 140, Loss: 0.20304, Train Acc: 0.93014, Test Acc: 0.72072\n",
      "Epoch: 141, Loss: 0.21420, Train Acc: 0.93313, Test Acc: 0.73874\n",
      "Epoch: 142, Loss: 0.19753, Train Acc: 0.93413, Test Acc: 0.73874\n",
      "Epoch: 143, Loss: 0.20483, Train Acc: 0.94212, Test Acc: 0.72973\n",
      "Epoch: 144, Loss: 0.19896, Train Acc: 0.95609, Test Acc: 0.72072\n",
      "Epoch: 145, Loss: 0.17311, Train Acc: 0.96307, Test Acc: 0.68468\n",
      "Epoch: 146, Loss: 0.15830, Train Acc: 0.94910, Test Acc: 0.69369\n",
      "Epoch: 147, Loss: 0.18470, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 148, Loss: 0.18968, Train Acc: 0.95210, Test Acc: 0.70270\n",
      "Epoch: 149, Loss: 0.14472, Train Acc: 0.94611, Test Acc: 0.72072\n",
      "Epoch: 150, Loss: 0.16253, Train Acc: 0.96108, Test Acc: 0.74775\n",
      "Epoch: 151, Loss: 0.16207, Train Acc: 0.97206, Test Acc: 0.74775\n",
      "Epoch: 152, Loss: 0.15024, Train Acc: 0.95210, Test Acc: 0.72973\n",
      "Epoch: 153, Loss: 0.17165, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 154, Loss: 0.15945, Train Acc: 0.95908, Test Acc: 0.72072\n",
      "Epoch: 155, Loss: 0.18727, Train Acc: 0.94511, Test Acc: 0.71171\n",
      "Epoch: 156, Loss: 0.15213, Train Acc: 0.95409, Test Acc: 0.72973\n",
      "Epoch: 157, Loss: 0.15487, Train Acc: 0.93214, Test Acc: 0.68468\n",
      "Epoch: 158, Loss: 0.19028, Train Acc: 0.93912, Test Acc: 0.70270\n",
      "Epoch: 159, Loss: 0.15470, Train Acc: 0.93812, Test Acc: 0.66667\n",
      "Epoch: 160, Loss: 0.16697, Train Acc: 0.92814, Test Acc: 0.64865\n",
      "Epoch: 161, Loss: 0.18450, Train Acc: 0.94611, Test Acc: 0.67568\n",
      "Epoch: 162, Loss: 0.21105, Train Acc: 0.91218, Test Acc: 0.63063\n",
      "Epoch: 163, Loss: 0.19326, Train Acc: 0.90120, Test Acc: 0.71171\n",
      "Epoch: 164, Loss: 0.20128, Train Acc: 0.92715, Test Acc: 0.67568\n",
      "Epoch: 165, Loss: 0.20114, Train Acc: 0.92515, Test Acc: 0.66667\n",
      "Epoch: 166, Loss: 0.17511, Train Acc: 0.94611, Test Acc: 0.74775\n",
      "Epoch: 167, Loss: 0.15736, Train Acc: 0.93812, Test Acc: 0.73874\n",
      "Epoch: 168, Loss: 0.19465, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 169, Loss: 0.16225, Train Acc: 0.95210, Test Acc: 0.72072\n",
      "Epoch: 170, Loss: 0.18879, Train Acc: 0.93912, Test Acc: 0.75676\n",
      "Epoch: 171, Loss: 0.19217, Train Acc: 0.91118, Test Acc: 0.72072\n",
      "Epoch: 172, Loss: 0.19118, Train Acc: 0.95309, Test Acc: 0.72072\n",
      "Epoch: 173, Loss: 0.17934, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 174, Loss: 0.15845, Train Acc: 0.95908, Test Acc: 0.72973\n",
      "Epoch: 175, Loss: 0.18280, Train Acc: 0.95110, Test Acc: 0.69369\n",
      "Epoch: 176, Loss: 0.20469, Train Acc: 0.93313, Test Acc: 0.67568\n",
      "Epoch: 177, Loss: 0.22061, Train Acc: 0.96407, Test Acc: 0.67568\n",
      "Epoch: 178, Loss: 0.15292, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 179, Loss: 0.15615, Train Acc: 0.95110, Test Acc: 0.75676\n",
      "Epoch: 180, Loss: 0.15942, Train Acc: 0.96707, Test Acc: 0.76577\n",
      "Epoch: 181, Loss: 0.14494, Train Acc: 0.96208, Test Acc: 0.73874\n",
      "Epoch: 182, Loss: 0.13089, Train Acc: 0.97206, Test Acc: 0.73874\n",
      "Epoch: 183, Loss: 0.12631, Train Acc: 0.96607, Test Acc: 0.73874\n",
      "Epoch: 184, Loss: 0.12990, Train Acc: 0.94910, Test Acc: 0.73874\n",
      "Epoch: 185, Loss: 0.14997, Train Acc: 0.96607, Test Acc: 0.75676\n",
      "Epoch: 186, Loss: 0.11839, Train Acc: 0.97605, Test Acc: 0.74775\n",
      "Epoch: 187, Loss: 0.10730, Train Acc: 0.98104, Test Acc: 0.71171\n",
      "Epoch: 188, Loss: 0.12760, Train Acc: 0.98104, Test Acc: 0.69369\n",
      "Epoch: 189, Loss: 0.10207, Train Acc: 0.98303, Test Acc: 0.75676\n",
      "Epoch: 190, Loss: 0.10017, Train Acc: 0.98204, Test Acc: 0.75676\n",
      "Epoch: 191, Loss: 0.11475, Train Acc: 0.98603, Test Acc: 0.68468\n",
      "Epoch: 192, Loss: 0.10596, Train Acc: 0.98104, Test Acc: 0.70270\n",
      "Epoch: 193, Loss: 0.11342, Train Acc: 0.96507, Test Acc: 0.71171\n",
      "Epoch: 194, Loss: 0.13239, Train Acc: 0.97505, Test Acc: 0.74775\n",
      "Epoch: 195, Loss: 0.10453, Train Acc: 0.97605, Test Acc: 0.72973\n",
      "Epoch: 196, Loss: 0.10578, Train Acc: 0.95309, Test Acc: 0.70270\n",
      "Epoch: 197, Loss: 0.11440, Train Acc: 0.96906, Test Acc: 0.68468\n",
      "Epoch: 198, Loss: 0.13735, Train Acc: 0.95709, Test Acc: 0.74775\n",
      "Epoch: 199, Loss: 0.14729, Train Acc: 0.96507, Test Acc: 0.76577\n",
      "Epoch: 200, Loss: 0.11679, Train Acc: 0.97405, Test Acc: 0.71171\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "\n",
    "path = 'data/TUDataset'\n",
    "dataset = TUDataset(path, name='PROTEINS')\n",
    "dataset = dataset.shuffle()\n",
    "n = len(dataset) // 10\n",
    "test_dataset = dataset[:n]\n",
    "train_dataset = dataset[n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=60)\n",
    "train_loader = DataLoader(train_dataset, batch_size=60)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(dataset.num_features, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, '\n",
    "          f'Test Acc: {test_acc:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|█         | 1/10 [00:00<00:00,  9.78number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|███       | 3/10 [00:00<00:00,  9.25number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|█████     | 5/10 [00:00<00:00,  9.19number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|███████   | 7/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  90%|█████████ | 9/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 10/10 [00:01<00:00,  9.19number/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in tqdm(range(1, n+1), desc='Progress', unit='number'):\n",
    "    print(i)\n",
    "    time.sleep(0.1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance: tensor([[ 1.2275e-02, -4.3866e-04, -2.3747e-03,  5.9961e-06, -1.8322e-02,\n",
      "          6.7758e-03, -3.3756e-03,  3.3045e-03, -4.1410e-04, -4.7517e-03,\n",
      "          6.5441e-03,  1.1756e-04,  8.9861e-03, -6.2483e-03, -2.0754e-04],\n",
      "        [ 9.7265e-04, -4.1853e-04,  8.4249e-04, -4.1004e-04, -1.3501e-03,\n",
      "         -2.3933e-04, -1.1280e-03, -7.0684e-04,  3.6807e-04, -7.3343e-04,\n",
      "          6.7297e-04,  1.7970e-05,  6.3480e-04, -1.7746e-04, -7.7775e-04],\n",
      "        [ 7.6379e-04, -4.1705e-04,  5.0388e-03,  9.4725e-03,  1.3427e-03,\n",
      "         -3.9083e-03, -7.4786e-04, -9.5126e-03,  2.7012e-03, -1.3322e-02,\n",
      "         -2.7063e-03, -5.9831e-03, -1.0255e-02,  6.4585e-03, -2.2670e-03],\n",
      "        [ 2.3280e-02, -1.9069e-03,  1.2953e-02, -1.7481e-03, -2.7319e-03,\n",
      "          2.4886e-02, -1.2913e-02,  4.1282e-04, -2.1938e-02,  1.2066e-02,\n",
      "          9.2277e-03,  1.7172e-02,  2.6745e-02, -1.2474e-02, -8.2982e-03],\n",
      "        [ 7.0498e-03, -1.2809e-03, -4.0443e-03,  5.3280e-03, -1.0616e-03,\n",
      "          5.3567e-03, -1.1408e-02,  4.7099e-03,  2.3087e-03, -2.3428e-03,\n",
      "         -3.9236e-03,  3.3648e-03,  1.6152e-02, -1.9746e-03, -3.1419e-03],\n",
      "        [ 2.0292e-03,  1.5952e-04, -1.1887e-03,  5.4479e-04, -5.4863e-04,\n",
      "          1.9303e-03, -1.4612e-03,  2.2287e-03, -1.4091e-03,  4.1199e-04,\n",
      "         -7.8360e-04,  9.2731e-04,  2.3172e-03, -7.5774e-04, -1.5543e-03],\n",
      "        [ 8.9949e-03,  8.8533e-04, -2.1451e-03,  7.0428e-03,  4.7685e-03,\n",
      "          5.8864e-03, -6.4042e-03,  3.9720e-03, -5.3514e-03, -1.2071e-03,\n",
      "         -1.4672e-03,  4.7018e-03,  7.4761e-03, -1.3038e-03, -1.2246e-02],\n",
      "        [-6.6790e-03, -4.5793e-03, -1.3295e-02, -4.9643e-04,  9.9808e-03,\n",
      "         -1.7760e-02,  3.9535e-03, -3.5023e-03,  1.9258e-02, -1.2740e-03,\n",
      "          2.0336e-03, -5.7606e-03, -5.5823e-03, -3.2434e-03,  1.3776e-03],\n",
      "        [-4.9399e-03, -3.0927e-03,  3.5786e-03, -3.8228e-03,  1.2745e-03,\n",
      "         -5.3330e-03, -1.1231e-03, -2.8851e-03,  4.4198e-03,  7.0476e-04,\n",
      "         -2.8511e-03, -1.5016e-03, -4.2478e-03,  4.3503e-03,  5.4290e-03],\n",
      "        [-1.1208e-03, -3.3793e-05,  3.4144e-04, -5.5016e-04,  1.5730e-04,\n",
      "         -6.2237e-04,  6.3486e-04, -3.7564e-04,  4.7872e-04,  3.7283e-04,\n",
      "         -6.9144e-07, -2.4358e-04, -8.4744e-04, -1.0327e-04,  7.2700e-04],\n",
      "        [ 7.5913e-03, -3.6760e-04,  2.5405e-04,  1.4391e-02, -7.0900e-03,\n",
      "          9.2784e-03, -2.1551e-03,  1.4613e-02, -8.1834e-03, -8.6680e-03,\n",
      "         -6.1243e-03, -1.5186e-04,  2.0315e-02, -8.2295e-03, -1.3247e-02],\n",
      "        [-3.6148e-04,  1.8751e-04, -7.9749e-05, -1.9293e-04, -9.9177e-05,\n",
      "         -3.3470e-04,  4.6496e-05,  3.7124e-05,  3.1666e-04,  4.6569e-04,\n",
      "         -2.0883e-04,  3.1561e-05, -4.3384e-04, -5.1910e-05,  5.3931e-04],\n",
      "        [ 5.4399e-03,  3.3108e-03, -5.0663e-03, -1.5073e-04,  1.5731e-03,\n",
      "          1.9585e-03, -1.6530e-03,  2.5281e-03, -2.4772e-03,  1.0186e-03,\n",
      "         -4.4670e-04,  1.4380e-03,  1.3553e-03, -2.0077e-04,  1.7682e-03],\n",
      "        [-3.7684e-04,  1.4193e-03,  2.0130e-03,  8.5876e-04, -2.0750e-03,\n",
      "          4.1714e-03, -2.5274e-04,  3.3690e-03, -3.6807e-03, -1.2927e-03,\n",
      "          5.9008e-06,  5.0497e-04,  2.1869e-03, -2.3429e-03, -2.2337e-03],\n",
      "        [-1.8282e-02,  9.8750e-03,  9.4390e-03, -6.5219e-03, -4.1733e-03,\n",
      "         -9.3553e-03,  1.7525e-02, -2.4783e-03, -9.8174e-03,  1.2298e-02,\n",
      "          1.1746e-02,  1.1692e-03, -1.3515e-02,  7.1292e-03,  1.6060e-02],\n",
      "        [ 1.7786e-03, -1.1181e-03,  9.1284e-05, -3.2955e-04, -1.9153e-03,\n",
      "         -1.1905e-03, -1.4411e-03, -2.0828e-04,  9.0699e-04, -1.7265e-03,\n",
      "         -5.3761e-04,  5.5327e-04,  2.0214e-03, -4.0939e-04, -2.7614e-04],\n",
      "        [ 1.0221e-02, -3.5139e-03, -2.9626e-03,  6.8862e-03, -1.3246e-02,\n",
      "          2.3670e-03, -5.1162e-03,  3.1607e-03, -4.1904e-03, -1.3504e-02,\n",
      "         -1.6872e-03,  2.1691e-03,  2.1273e-02, -2.9301e-03, -1.5891e-02],\n",
      "        [ 5.3006e-03,  1.3953e-03, -1.4417e-03,  1.0278e-03,  2.5639e-05,\n",
      "          2.8371e-03,  3.2918e-04, -9.2075e-04, -1.7274e-03, -5.2760e-04,\n",
      "          1.0792e-03, -1.3019e-04, -1.2930e-04,  7.9166e-05,  2.3511e-04],\n",
      "        [-1.0612e-02, -2.1024e-02,  5.7821e-03, -2.2025e-02,  1.7100e-03,\n",
      "         -3.3466e-02, -1.3780e-02, -1.8313e-02,  2.8295e-02,  7.7237e-03,\n",
      "         -1.3175e-02, -1.3969e-02, -1.6624e-02,  1.2225e-02,  2.6744e-02],\n",
      "        [ 3.8612e-03,  2.0107e-03,  1.6856e-03, -3.5960e-04, -1.3485e-02,\n",
      "          9.4410e-03,  1.2272e-03,  6.6000e-03, -5.0133e-03, -5.0718e-03,\n",
      "          4.1998e-03,  2.1863e-03,  4.9845e-03, -3.0835e-03, -3.4995e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Define a simple GAT-based GNN model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_heads):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_size, hidden_size, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_size * num_heads, output_size, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Function to compute feature importance using gradient\n",
    "def compute_feature_importance_gat(model, data, target):\n",
    "    model.eval()\n",
    "    data.x.requires_grad_(True)\n",
    "\n",
    "    optimizer = optim.SGD([data.x], lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Compute feature importance as the absolute gradient\n",
    "    # feature_importance = torch.abs(data.x.grad)\n",
    "    feature_importance = data.x.grad\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "# Example usage\n",
    "# Assume your GNN model takes a graph as input, represented using the PyTorch Geometric data structure.\n",
    "# Here, we use a simple example with a random graph.\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "input_size=15\n",
    "output_size=1\n",
    "hidden_size=64\n",
    "# Generate a random graph\n",
    "num_nodes = 20\n",
    "num_edges = 15\n",
    "data = Data(x=torch.randn((num_nodes, input_size), requires_grad=True),\n",
    "            edge_index=torch.randint(0, num_nodes, (2, num_edges), dtype=torch.long),\n",
    "            y=torch.randn((num_nodes, output_size)))\n",
    "\n",
    "# Initialize the GAT-based GNN model\n",
    "gat_model = GATModel(input_size, hidden_size, output_size, num_heads=2)\n",
    "\n",
    "# Compute feature importance\n",
    "feature_importance = compute_feature_importance_gat(gat_model, data, data.y)\n",
    "\n",
    "print(\"Feature Importance:\", feature_importance)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:56.586341300Z",
     "start_time": "2023-11-14T08:04:56.559131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(feature_importance))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:57.054878800Z",
     "start_time": "2023-11-14T08:04:57.038416500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 15])\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:01:52.293875300Z",
     "start_time": "2023-11-14T08:01:52.278989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_19068\\1474834421.py\", line 48, in <module>\n",
      "    explainer = shap.DeepExplainer(model_predict, loader)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py\", line 84, in __init__\n",
      "    self.explainer = TFDeep(model, data, session, learning_phase_flags)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 110, in __init__\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\tf_utils.py\", line 69, in _get_model_inputs\n",
      "AssertionError: <class 'function'> is not currently a supported model type!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim as optim\n",
    "import shap\n",
    "\n",
    "# Define a simple PyTorch GNN model for graph regression with GCNConv\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create a dummy graph dataset\n",
    "# Note: In a real-world scenario, you would load your dataset using PyG's available datasets.\n",
    "x = torch.randn((5, 1))  # Node features (5 nodes, 1 feature per node)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 3, 4], [1, 0, 2, 1, 3, 2, 4, 3]], dtype=torch.long)\n",
    "y = torch.randn((5, 1))  # Target values (regression task)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "\n",
    "# Create a simple GNN model with GCNConv\n",
    "model = SimpleGNN(in_feats=1, hidden_size=16, out_feats=1)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Define a function for the GNN model\n",
    "def model_predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "# Wrap the model_predict function with a shap.DeepExplainer\n",
    "explainer = shap.DeepExplainer(model_predict, loader)\n",
    "\n",
    "# Get Shapley values for a specific data point\n",
    "shap_values = explainer.shap_values(data)\n",
    "\n",
    "# Print Shapley values for each feature\n",
    "print(\"Shapley Values:\", shap_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T06:10:52.811355900Z",
     "start_time": "2023-12-07T06:10:52.749280700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\3908104130.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPlanetoid\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExplainer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mGNNExplainer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGCNConv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 10\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "\n",
    "path = 'feature_importance.png'\n",
    "explanation.visualize_feature_importance(path, top_k=10)\n",
    "print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "path = 'subgraph.pdf'\n",
    "explanation.visualize_graph(path)\n",
    "print(f\"Subgraph visualization plot has been saved to '{path}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T03:38:35.525831900Z",
     "start_time": "2023-12-20T03:38:31.357576100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\52961935.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTUDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T11:05:47.687249Z",
     "start_time": "2023-12-20T11:05:47.457318700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2101667002.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_15604\\2101667002.py\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\u001B[0m\n\u001B[1;37m                                                                      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\n",
    "    {'gnnConv1': ('GATConv', 1, 1), 'gnnConv2': ('LEConv', 13, 6), ... 'KendalTau': {0}},\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4851}},\n",
    "    {'gnnConv1': ('SGConv', 4, 4), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4852}},\n",
    "    {'gnnConv1': ('LEConv', 6, 6), 'gnnConv2': ('GCNConv', 7, 0), ... 'KendalTau': {0.2727}}\n",
    "]\n",
    "\n",
    "max_kendaltau_entry = max(data, key=lambda x: x['KendalTau'].pop())  # Pop the only element from the set\n",
    "\n",
    "print(\"Entry with the highest KendalTau:\")\n",
    "print(max_kendaltau_entry)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T21:49:04.345165800Z",
     "start_time": "2023-12-21T21:49:04.256696400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
