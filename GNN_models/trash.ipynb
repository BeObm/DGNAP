{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.63994, Train Acc: 0.67166, Test Acc: 0.77477\n",
      "Epoch: 002, Loss: 0.60602, Train Acc: 0.72555, Test Acc: 0.75676\n",
      "Epoch: 003, Loss: 0.59269, Train Acc: 0.73553, Test Acc: 0.78378\n",
      "Epoch: 004, Loss: 0.57202, Train Acc: 0.73353, Test Acc: 0.75676\n",
      "Epoch: 005, Loss: 0.56646, Train Acc: 0.73553, Test Acc: 0.72973\n",
      "Epoch: 006, Loss: 0.56377, Train Acc: 0.74451, Test Acc: 0.76577\n",
      "Epoch: 007, Loss: 0.53971, Train Acc: 0.74950, Test Acc: 0.77477\n",
      "Epoch: 008, Loss: 0.54838, Train Acc: 0.74750, Test Acc: 0.76577\n",
      "Epoch: 009, Loss: 0.54948, Train Acc: 0.75349, Test Acc: 0.75676\n",
      "Epoch: 010, Loss: 0.53813, Train Acc: 0.75349, Test Acc: 0.76577\n",
      "Epoch: 011, Loss: 0.53843, Train Acc: 0.75150, Test Acc: 0.76577\n",
      "Epoch: 012, Loss: 0.53526, Train Acc: 0.75649, Test Acc: 0.74775\n",
      "Epoch: 013, Loss: 0.53497, Train Acc: 0.76347, Test Acc: 0.76577\n",
      "Epoch: 014, Loss: 0.52893, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 015, Loss: 0.52412, Train Acc: 0.75848, Test Acc: 0.74775\n",
      "Epoch: 016, Loss: 0.52143, Train Acc: 0.75749, Test Acc: 0.73874\n",
      "Epoch: 017, Loss: 0.51217, Train Acc: 0.76248, Test Acc: 0.76577\n",
      "Epoch: 018, Loss: 0.50869, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 019, Loss: 0.50850, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 020, Loss: 0.50986, Train Acc: 0.76946, Test Acc: 0.73874\n",
      "Epoch: 021, Loss: 0.50446, Train Acc: 0.76647, Test Acc: 0.74775\n",
      "Epoch: 022, Loss: 0.50306, Train Acc: 0.77445, Test Acc: 0.74775\n",
      "Epoch: 023, Loss: 0.50349, Train Acc: 0.77046, Test Acc: 0.73874\n",
      "Epoch: 024, Loss: 0.50693, Train Acc: 0.77246, Test Acc: 0.75676\n",
      "Epoch: 025, Loss: 0.50562, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 026, Loss: 0.50851, Train Acc: 0.77645, Test Acc: 0.75676\n",
      "Epoch: 027, Loss: 0.51166, Train Acc: 0.77445, Test Acc: 0.75676\n",
      "Epoch: 028, Loss: 0.50159, Train Acc: 0.77944, Test Acc: 0.75676\n",
      "Epoch: 029, Loss: 0.49367, Train Acc: 0.78044, Test Acc: 0.74775\n",
      "Epoch: 030, Loss: 0.49450, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 031, Loss: 0.48509, Train Acc: 0.78842, Test Acc: 0.77477\n",
      "Epoch: 032, Loss: 0.49752, Train Acc: 0.78842, Test Acc: 0.75676\n",
      "Epoch: 033, Loss: 0.47788, Train Acc: 0.79242, Test Acc: 0.75676\n",
      "Epoch: 034, Loss: 0.47482, Train Acc: 0.78842, Test Acc: 0.74775\n",
      "Epoch: 035, Loss: 0.48345, Train Acc: 0.79042, Test Acc: 0.75676\n",
      "Epoch: 036, Loss: 0.47648, Train Acc: 0.78643, Test Acc: 0.76577\n",
      "Epoch: 037, Loss: 0.47779, Train Acc: 0.79341, Test Acc: 0.77477\n",
      "Epoch: 038, Loss: 0.47345, Train Acc: 0.79042, Test Acc: 0.77477\n",
      "Epoch: 039, Loss: 0.47719, Train Acc: 0.79142, Test Acc: 0.75676\n",
      "Epoch: 040, Loss: 0.46943, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 041, Loss: 0.45909, Train Acc: 0.79242, Test Acc: 0.79279\n",
      "Epoch: 042, Loss: 0.45949, Train Acc: 0.78942, Test Acc: 0.75676\n",
      "Epoch: 043, Loss: 0.47938, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 044, Loss: 0.45838, Train Acc: 0.80040, Test Acc: 0.77477\n",
      "Epoch: 045, Loss: 0.44669, Train Acc: 0.80140, Test Acc: 0.78378\n",
      "Epoch: 046, Loss: 0.45047, Train Acc: 0.79840, Test Acc: 0.78378\n",
      "Epoch: 047, Loss: 0.46048, Train Acc: 0.80040, Test Acc: 0.75676\n",
      "Epoch: 048, Loss: 0.45371, Train Acc: 0.79840, Test Acc: 0.75676\n",
      "Epoch: 049, Loss: 0.45315, Train Acc: 0.79341, Test Acc: 0.75676\n",
      "Epoch: 050, Loss: 0.44780, Train Acc: 0.79341, Test Acc: 0.76577\n",
      "Epoch: 051, Loss: 0.44573, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 052, Loss: 0.43719, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 053, Loss: 0.45039, Train Acc: 0.80838, Test Acc: 0.76577\n",
      "Epoch: 054, Loss: 0.44865, Train Acc: 0.80339, Test Acc: 0.78378\n",
      "Epoch: 055, Loss: 0.43105, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 056, Loss: 0.43284, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 057, Loss: 0.42553, Train Acc: 0.81238, Test Acc: 0.77477\n",
      "Epoch: 058, Loss: 0.42388, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 059, Loss: 0.42252, Train Acc: 0.80240, Test Acc: 0.74775\n",
      "Epoch: 060, Loss: 0.42339, Train Acc: 0.81337, Test Acc: 0.76577\n",
      "Epoch: 061, Loss: 0.41549, Train Acc: 0.81138, Test Acc: 0.76577\n",
      "Epoch: 062, Loss: 0.41829, Train Acc: 0.81038, Test Acc: 0.75676\n",
      "Epoch: 063, Loss: 0.41101, Train Acc: 0.81038, Test Acc: 0.74775\n",
      "Epoch: 064, Loss: 0.41326, Train Acc: 0.81537, Test Acc: 0.73874\n",
      "Epoch: 065, Loss: 0.40395, Train Acc: 0.80439, Test Acc: 0.75676\n",
      "Epoch: 066, Loss: 0.40527, Train Acc: 0.81936, Test Acc: 0.76577\n",
      "Epoch: 067, Loss: 0.40050, Train Acc: 0.82435, Test Acc: 0.76577\n",
      "Epoch: 068, Loss: 0.39777, Train Acc: 0.82036, Test Acc: 0.76577\n",
      "Epoch: 069, Loss: 0.39965, Train Acc: 0.82635, Test Acc: 0.73874\n",
      "Epoch: 070, Loss: 0.38398, Train Acc: 0.83034, Test Acc: 0.73874\n",
      "Epoch: 071, Loss: 0.39805, Train Acc: 0.82236, Test Acc: 0.74775\n",
      "Epoch: 072, Loss: 0.39025, Train Acc: 0.81836, Test Acc: 0.75676\n",
      "Epoch: 073, Loss: 0.41702, Train Acc: 0.80439, Test Acc: 0.77477\n",
      "Epoch: 074, Loss: 0.41968, Train Acc: 0.81537, Test Acc: 0.76577\n",
      "Epoch: 075, Loss: 0.39959, Train Acc: 0.82735, Test Acc: 0.76577\n",
      "Epoch: 076, Loss: 0.37986, Train Acc: 0.82735, Test Acc: 0.73874\n",
      "Epoch: 077, Loss: 0.38195, Train Acc: 0.83433, Test Acc: 0.76577\n",
      "Epoch: 078, Loss: 0.36943, Train Acc: 0.83733, Test Acc: 0.74775\n",
      "Epoch: 079, Loss: 0.35535, Train Acc: 0.82535, Test Acc: 0.72973\n",
      "Epoch: 080, Loss: 0.37084, Train Acc: 0.82136, Test Acc: 0.72072\n",
      "Epoch: 081, Loss: 0.37048, Train Acc: 0.83034, Test Acc: 0.72973\n",
      "Epoch: 082, Loss: 0.36895, Train Acc: 0.82735, Test Acc: 0.69369\n",
      "Epoch: 083, Loss: 0.37141, Train Acc: 0.83533, Test Acc: 0.76577\n",
      "Epoch: 084, Loss: 0.35657, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 085, Loss: 0.35072, Train Acc: 0.82934, Test Acc: 0.70270\n",
      "Epoch: 086, Loss: 0.35426, Train Acc: 0.84331, Test Acc: 0.72973\n",
      "Epoch: 087, Loss: 0.34678, Train Acc: 0.85130, Test Acc: 0.70270\n",
      "Epoch: 088, Loss: 0.33941, Train Acc: 0.84531, Test Acc: 0.72973\n",
      "Epoch: 089, Loss: 0.33547, Train Acc: 0.85329, Test Acc: 0.73874\n",
      "Epoch: 090, Loss: 0.32926, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 091, Loss: 0.31867, Train Acc: 0.86327, Test Acc: 0.73874\n",
      "Epoch: 092, Loss: 0.32957, Train Acc: 0.86128, Test Acc: 0.70270\n",
      "Epoch: 093, Loss: 0.31665, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 094, Loss: 0.31365, Train Acc: 0.85429, Test Acc: 0.76577\n",
      "Epoch: 095, Loss: 0.32666, Train Acc: 0.85928, Test Acc: 0.74775\n",
      "Epoch: 096, Loss: 0.32321, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 097, Loss: 0.32056, Train Acc: 0.87425, Test Acc: 0.77477\n",
      "Epoch: 098, Loss: 0.30211, Train Acc: 0.86627, Test Acc: 0.73874\n",
      "Epoch: 099, Loss: 0.31116, Train Acc: 0.85928, Test Acc: 0.78378\n",
      "Epoch: 100, Loss: 0.30914, Train Acc: 0.87625, Test Acc: 0.76577\n",
      "Epoch: 101, Loss: 0.31156, Train Acc: 0.87625, Test Acc: 0.75676\n",
      "Epoch: 102, Loss: 0.29537, Train Acc: 0.87824, Test Acc: 0.72072\n",
      "Epoch: 103, Loss: 0.30042, Train Acc: 0.88723, Test Acc: 0.74775\n",
      "Epoch: 104, Loss: 0.28999, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 105, Loss: 0.28154, Train Acc: 0.86427, Test Acc: 0.74775\n",
      "Epoch: 106, Loss: 0.27667, Train Acc: 0.88523, Test Acc: 0.77477\n",
      "Epoch: 107, Loss: 0.26873, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 108, Loss: 0.28136, Train Acc: 0.89022, Test Acc: 0.77477\n",
      "Epoch: 109, Loss: 0.27982, Train Acc: 0.89421, Test Acc: 0.76577\n",
      "Epoch: 110, Loss: 0.26023, Train Acc: 0.90519, Test Acc: 0.72072\n",
      "Epoch: 111, Loss: 0.26328, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 112, Loss: 0.25424, Train Acc: 0.89621, Test Acc: 0.73874\n",
      "Epoch: 113, Loss: 0.25966, Train Acc: 0.89421, Test Acc: 0.78378\n",
      "Epoch: 114, Loss: 0.24187, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 115, Loss: 0.25082, Train Acc: 0.89820, Test Acc: 0.77477\n",
      "Epoch: 116, Loss: 0.28134, Train Acc: 0.91317, Test Acc: 0.73874\n",
      "Epoch: 117, Loss: 0.24313, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 118, Loss: 0.25145, Train Acc: 0.90918, Test Acc: 0.72973\n",
      "Epoch: 119, Loss: 0.26201, Train Acc: 0.90319, Test Acc: 0.72973\n",
      "Epoch: 120, Loss: 0.23378, Train Acc: 0.91617, Test Acc: 0.69369\n",
      "Epoch: 121, Loss: 0.24473, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 122, Loss: 0.23172, Train Acc: 0.91517, Test Acc: 0.75676\n",
      "Epoch: 123, Loss: 0.23335, Train Acc: 0.92016, Test Acc: 0.75676\n",
      "Epoch: 124, Loss: 0.20975, Train Acc: 0.92216, Test Acc: 0.73874\n",
      "Epoch: 125, Loss: 0.20874, Train Acc: 0.92515, Test Acc: 0.76577\n",
      "Epoch: 126, Loss: 0.20907, Train Acc: 0.93014, Test Acc: 0.77477\n",
      "Epoch: 127, Loss: 0.20823, Train Acc: 0.92515, Test Acc: 0.70270\n",
      "Epoch: 128, Loss: 0.20784, Train Acc: 0.93513, Test Acc: 0.73874\n",
      "Epoch: 129, Loss: 0.20660, Train Acc: 0.93812, Test Acc: 0.74775\n",
      "Epoch: 130, Loss: 0.22070, Train Acc: 0.92914, Test Acc: 0.71171\n",
      "Epoch: 131, Loss: 0.22617, Train Acc: 0.93413, Test Acc: 0.78378\n",
      "Epoch: 132, Loss: 0.20191, Train Acc: 0.93513, Test Acc: 0.71171\n",
      "Epoch: 133, Loss: 0.20896, Train Acc: 0.93613, Test Acc: 0.76577\n",
      "Epoch: 134, Loss: 0.22466, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 135, Loss: 0.24093, Train Acc: 0.93613, Test Acc: 0.66667\n",
      "Epoch: 136, Loss: 0.21365, Train Acc: 0.93812, Test Acc: 0.70270\n",
      "Epoch: 137, Loss: 0.22885, Train Acc: 0.92016, Test Acc: 0.73874\n",
      "Epoch: 138, Loss: 0.23151, Train Acc: 0.92016, Test Acc: 0.69369\n",
      "Epoch: 139, Loss: 0.23535, Train Acc: 0.92715, Test Acc: 0.72973\n",
      "Epoch: 140, Loss: 0.20304, Train Acc: 0.93014, Test Acc: 0.72072\n",
      "Epoch: 141, Loss: 0.21420, Train Acc: 0.93313, Test Acc: 0.73874\n",
      "Epoch: 142, Loss: 0.19753, Train Acc: 0.93413, Test Acc: 0.73874\n",
      "Epoch: 143, Loss: 0.20483, Train Acc: 0.94212, Test Acc: 0.72973\n",
      "Epoch: 144, Loss: 0.19896, Train Acc: 0.95609, Test Acc: 0.72072\n",
      "Epoch: 145, Loss: 0.17311, Train Acc: 0.96307, Test Acc: 0.68468\n",
      "Epoch: 146, Loss: 0.15830, Train Acc: 0.94910, Test Acc: 0.69369\n",
      "Epoch: 147, Loss: 0.18470, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 148, Loss: 0.18968, Train Acc: 0.95210, Test Acc: 0.70270\n",
      "Epoch: 149, Loss: 0.14472, Train Acc: 0.94611, Test Acc: 0.72072\n",
      "Epoch: 150, Loss: 0.16253, Train Acc: 0.96108, Test Acc: 0.74775\n",
      "Epoch: 151, Loss: 0.16207, Train Acc: 0.97206, Test Acc: 0.74775\n",
      "Epoch: 152, Loss: 0.15024, Train Acc: 0.95210, Test Acc: 0.72973\n",
      "Epoch: 153, Loss: 0.17165, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 154, Loss: 0.15945, Train Acc: 0.95908, Test Acc: 0.72072\n",
      "Epoch: 155, Loss: 0.18727, Train Acc: 0.94511, Test Acc: 0.71171\n",
      "Epoch: 156, Loss: 0.15213, Train Acc: 0.95409, Test Acc: 0.72973\n",
      "Epoch: 157, Loss: 0.15487, Train Acc: 0.93214, Test Acc: 0.68468\n",
      "Epoch: 158, Loss: 0.19028, Train Acc: 0.93912, Test Acc: 0.70270\n",
      "Epoch: 159, Loss: 0.15470, Train Acc: 0.93812, Test Acc: 0.66667\n",
      "Epoch: 160, Loss: 0.16697, Train Acc: 0.92814, Test Acc: 0.64865\n",
      "Epoch: 161, Loss: 0.18450, Train Acc: 0.94611, Test Acc: 0.67568\n",
      "Epoch: 162, Loss: 0.21105, Train Acc: 0.91218, Test Acc: 0.63063\n",
      "Epoch: 163, Loss: 0.19326, Train Acc: 0.90120, Test Acc: 0.71171\n",
      "Epoch: 164, Loss: 0.20128, Train Acc: 0.92715, Test Acc: 0.67568\n",
      "Epoch: 165, Loss: 0.20114, Train Acc: 0.92515, Test Acc: 0.66667\n",
      "Epoch: 166, Loss: 0.17511, Train Acc: 0.94611, Test Acc: 0.74775\n",
      "Epoch: 167, Loss: 0.15736, Train Acc: 0.93812, Test Acc: 0.73874\n",
      "Epoch: 168, Loss: 0.19465, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 169, Loss: 0.16225, Train Acc: 0.95210, Test Acc: 0.72072\n",
      "Epoch: 170, Loss: 0.18879, Train Acc: 0.93912, Test Acc: 0.75676\n",
      "Epoch: 171, Loss: 0.19217, Train Acc: 0.91118, Test Acc: 0.72072\n",
      "Epoch: 172, Loss: 0.19118, Train Acc: 0.95309, Test Acc: 0.72072\n",
      "Epoch: 173, Loss: 0.17934, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 174, Loss: 0.15845, Train Acc: 0.95908, Test Acc: 0.72973\n",
      "Epoch: 175, Loss: 0.18280, Train Acc: 0.95110, Test Acc: 0.69369\n",
      "Epoch: 176, Loss: 0.20469, Train Acc: 0.93313, Test Acc: 0.67568\n",
      "Epoch: 177, Loss: 0.22061, Train Acc: 0.96407, Test Acc: 0.67568\n",
      "Epoch: 178, Loss: 0.15292, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 179, Loss: 0.15615, Train Acc: 0.95110, Test Acc: 0.75676\n",
      "Epoch: 180, Loss: 0.15942, Train Acc: 0.96707, Test Acc: 0.76577\n",
      "Epoch: 181, Loss: 0.14494, Train Acc: 0.96208, Test Acc: 0.73874\n",
      "Epoch: 182, Loss: 0.13089, Train Acc: 0.97206, Test Acc: 0.73874\n",
      "Epoch: 183, Loss: 0.12631, Train Acc: 0.96607, Test Acc: 0.73874\n",
      "Epoch: 184, Loss: 0.12990, Train Acc: 0.94910, Test Acc: 0.73874\n",
      "Epoch: 185, Loss: 0.14997, Train Acc: 0.96607, Test Acc: 0.75676\n",
      "Epoch: 186, Loss: 0.11839, Train Acc: 0.97605, Test Acc: 0.74775\n",
      "Epoch: 187, Loss: 0.10730, Train Acc: 0.98104, Test Acc: 0.71171\n",
      "Epoch: 188, Loss: 0.12760, Train Acc: 0.98104, Test Acc: 0.69369\n",
      "Epoch: 189, Loss: 0.10207, Train Acc: 0.98303, Test Acc: 0.75676\n",
      "Epoch: 190, Loss: 0.10017, Train Acc: 0.98204, Test Acc: 0.75676\n",
      "Epoch: 191, Loss: 0.11475, Train Acc: 0.98603, Test Acc: 0.68468\n",
      "Epoch: 192, Loss: 0.10596, Train Acc: 0.98104, Test Acc: 0.70270\n",
      "Epoch: 193, Loss: 0.11342, Train Acc: 0.96507, Test Acc: 0.71171\n",
      "Epoch: 194, Loss: 0.13239, Train Acc: 0.97505, Test Acc: 0.74775\n",
      "Epoch: 195, Loss: 0.10453, Train Acc: 0.97605, Test Acc: 0.72973\n",
      "Epoch: 196, Loss: 0.10578, Train Acc: 0.95309, Test Acc: 0.70270\n",
      "Epoch: 197, Loss: 0.11440, Train Acc: 0.96906, Test Acc: 0.68468\n",
      "Epoch: 198, Loss: 0.13735, Train Acc: 0.95709, Test Acc: 0.74775\n",
      "Epoch: 199, Loss: 0.14729, Train Acc: 0.96507, Test Acc: 0.76577\n",
      "Epoch: 200, Loss: 0.11679, Train Acc: 0.97405, Test Acc: 0.71171\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "\n",
    "path = 'data/TUDataset'\n",
    "dataset = TUDataset(path, name='PROTEINS')\n",
    "dataset = dataset.shuffle()\n",
    "n = len(dataset) // 10\n",
    "test_dataset = dataset[:n]\n",
    "train_dataset = dataset[n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=60)\n",
    "train_loader = DataLoader(train_dataset, batch_size=60)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(dataset.num_features, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, '\n",
    "          f'Test Acc: {test_acc:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|█         | 1/10 [00:00<00:00,  9.78number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|███       | 3/10 [00:00<00:00,  9.25number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|█████     | 5/10 [00:00<00:00,  9.19number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|███████   | 7/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  90%|█████████ | 9/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 10/10 [00:01<00:00,  9.19number/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in tqdm(range(1, n+1), desc='Progress', unit='number'):\n",
    "    print(i)\n",
    "    time.sleep(0.1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance: tensor([[ 1.2275e-02, -4.3866e-04, -2.3747e-03,  5.9961e-06, -1.8322e-02,\n",
      "          6.7758e-03, -3.3756e-03,  3.3045e-03, -4.1410e-04, -4.7517e-03,\n",
      "          6.5441e-03,  1.1756e-04,  8.9861e-03, -6.2483e-03, -2.0754e-04],\n",
      "        [ 9.7265e-04, -4.1853e-04,  8.4249e-04, -4.1004e-04, -1.3501e-03,\n",
      "         -2.3933e-04, -1.1280e-03, -7.0684e-04,  3.6807e-04, -7.3343e-04,\n",
      "          6.7297e-04,  1.7970e-05,  6.3480e-04, -1.7746e-04, -7.7775e-04],\n",
      "        [ 7.6379e-04, -4.1705e-04,  5.0388e-03,  9.4725e-03,  1.3427e-03,\n",
      "         -3.9083e-03, -7.4786e-04, -9.5126e-03,  2.7012e-03, -1.3322e-02,\n",
      "         -2.7063e-03, -5.9831e-03, -1.0255e-02,  6.4585e-03, -2.2670e-03],\n",
      "        [ 2.3280e-02, -1.9069e-03,  1.2953e-02, -1.7481e-03, -2.7319e-03,\n",
      "          2.4886e-02, -1.2913e-02,  4.1282e-04, -2.1938e-02,  1.2066e-02,\n",
      "          9.2277e-03,  1.7172e-02,  2.6745e-02, -1.2474e-02, -8.2982e-03],\n",
      "        [ 7.0498e-03, -1.2809e-03, -4.0443e-03,  5.3280e-03, -1.0616e-03,\n",
      "          5.3567e-03, -1.1408e-02,  4.7099e-03,  2.3087e-03, -2.3428e-03,\n",
      "         -3.9236e-03,  3.3648e-03,  1.6152e-02, -1.9746e-03, -3.1419e-03],\n",
      "        [ 2.0292e-03,  1.5952e-04, -1.1887e-03,  5.4479e-04, -5.4863e-04,\n",
      "          1.9303e-03, -1.4612e-03,  2.2287e-03, -1.4091e-03,  4.1199e-04,\n",
      "         -7.8360e-04,  9.2731e-04,  2.3172e-03, -7.5774e-04, -1.5543e-03],\n",
      "        [ 8.9949e-03,  8.8533e-04, -2.1451e-03,  7.0428e-03,  4.7685e-03,\n",
      "          5.8864e-03, -6.4042e-03,  3.9720e-03, -5.3514e-03, -1.2071e-03,\n",
      "         -1.4672e-03,  4.7018e-03,  7.4761e-03, -1.3038e-03, -1.2246e-02],\n",
      "        [-6.6790e-03, -4.5793e-03, -1.3295e-02, -4.9643e-04,  9.9808e-03,\n",
      "         -1.7760e-02,  3.9535e-03, -3.5023e-03,  1.9258e-02, -1.2740e-03,\n",
      "          2.0336e-03, -5.7606e-03, -5.5823e-03, -3.2434e-03,  1.3776e-03],\n",
      "        [-4.9399e-03, -3.0927e-03,  3.5786e-03, -3.8228e-03,  1.2745e-03,\n",
      "         -5.3330e-03, -1.1231e-03, -2.8851e-03,  4.4198e-03,  7.0476e-04,\n",
      "         -2.8511e-03, -1.5016e-03, -4.2478e-03,  4.3503e-03,  5.4290e-03],\n",
      "        [-1.1208e-03, -3.3793e-05,  3.4144e-04, -5.5016e-04,  1.5730e-04,\n",
      "         -6.2237e-04,  6.3486e-04, -3.7564e-04,  4.7872e-04,  3.7283e-04,\n",
      "         -6.9144e-07, -2.4358e-04, -8.4744e-04, -1.0327e-04,  7.2700e-04],\n",
      "        [ 7.5913e-03, -3.6760e-04,  2.5405e-04,  1.4391e-02, -7.0900e-03,\n",
      "          9.2784e-03, -2.1551e-03,  1.4613e-02, -8.1834e-03, -8.6680e-03,\n",
      "         -6.1243e-03, -1.5186e-04,  2.0315e-02, -8.2295e-03, -1.3247e-02],\n",
      "        [-3.6148e-04,  1.8751e-04, -7.9749e-05, -1.9293e-04, -9.9177e-05,\n",
      "         -3.3470e-04,  4.6496e-05,  3.7124e-05,  3.1666e-04,  4.6569e-04,\n",
      "         -2.0883e-04,  3.1561e-05, -4.3384e-04, -5.1910e-05,  5.3931e-04],\n",
      "        [ 5.4399e-03,  3.3108e-03, -5.0663e-03, -1.5073e-04,  1.5731e-03,\n",
      "          1.9585e-03, -1.6530e-03,  2.5281e-03, -2.4772e-03,  1.0186e-03,\n",
      "         -4.4670e-04,  1.4380e-03,  1.3553e-03, -2.0077e-04,  1.7682e-03],\n",
      "        [-3.7684e-04,  1.4193e-03,  2.0130e-03,  8.5876e-04, -2.0750e-03,\n",
      "          4.1714e-03, -2.5274e-04,  3.3690e-03, -3.6807e-03, -1.2927e-03,\n",
      "          5.9008e-06,  5.0497e-04,  2.1869e-03, -2.3429e-03, -2.2337e-03],\n",
      "        [-1.8282e-02,  9.8750e-03,  9.4390e-03, -6.5219e-03, -4.1733e-03,\n",
      "         -9.3553e-03,  1.7525e-02, -2.4783e-03, -9.8174e-03,  1.2298e-02,\n",
      "          1.1746e-02,  1.1692e-03, -1.3515e-02,  7.1292e-03,  1.6060e-02],\n",
      "        [ 1.7786e-03, -1.1181e-03,  9.1284e-05, -3.2955e-04, -1.9153e-03,\n",
      "         -1.1905e-03, -1.4411e-03, -2.0828e-04,  9.0699e-04, -1.7265e-03,\n",
      "         -5.3761e-04,  5.5327e-04,  2.0214e-03, -4.0939e-04, -2.7614e-04],\n",
      "        [ 1.0221e-02, -3.5139e-03, -2.9626e-03,  6.8862e-03, -1.3246e-02,\n",
      "          2.3670e-03, -5.1162e-03,  3.1607e-03, -4.1904e-03, -1.3504e-02,\n",
      "         -1.6872e-03,  2.1691e-03,  2.1273e-02, -2.9301e-03, -1.5891e-02],\n",
      "        [ 5.3006e-03,  1.3953e-03, -1.4417e-03,  1.0278e-03,  2.5639e-05,\n",
      "          2.8371e-03,  3.2918e-04, -9.2075e-04, -1.7274e-03, -5.2760e-04,\n",
      "          1.0792e-03, -1.3019e-04, -1.2930e-04,  7.9166e-05,  2.3511e-04],\n",
      "        [-1.0612e-02, -2.1024e-02,  5.7821e-03, -2.2025e-02,  1.7100e-03,\n",
      "         -3.3466e-02, -1.3780e-02, -1.8313e-02,  2.8295e-02,  7.7237e-03,\n",
      "         -1.3175e-02, -1.3969e-02, -1.6624e-02,  1.2225e-02,  2.6744e-02],\n",
      "        [ 3.8612e-03,  2.0107e-03,  1.6856e-03, -3.5960e-04, -1.3485e-02,\n",
      "          9.4410e-03,  1.2272e-03,  6.6000e-03, -5.0133e-03, -5.0718e-03,\n",
      "          4.1998e-03,  2.1863e-03,  4.9845e-03, -3.0835e-03, -3.4995e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Define a simple GAT-based GNN model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_heads):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_size, hidden_size, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_size * num_heads, output_size, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Function to compute feature importance using gradient\n",
    "def compute_feature_importance_gat(model, data, target):\n",
    "    model.eval()\n",
    "    data.x.requires_grad_(True)\n",
    "\n",
    "    optimizer = optim.SGD([data.x], lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Compute feature importance as the absolute gradient\n",
    "    # feature_importance = torch.abs(data.x.grad)\n",
    "    feature_importance = data.x.grad\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "# Example usage\n",
    "# Assume your GNN model takes a graph as input, represented using the PyTorch Geometric data structure.\n",
    "# Here, we use a simple example with a random graph.\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "input_size=15\n",
    "output_size=1\n",
    "hidden_size=64\n",
    "# Generate a random graph\n",
    "num_nodes = 20\n",
    "num_edges = 15\n",
    "data = Data(x=torch.randn((num_nodes, input_size), requires_grad=True),\n",
    "            edge_index=torch.randint(0, num_nodes, (2, num_edges), dtype=torch.long),\n",
    "            y=torch.randn((num_nodes, output_size)))\n",
    "\n",
    "# Initialize the GAT-based GNN model\n",
    "gat_model = GATModel(input_size, hidden_size, output_size, num_heads=2)\n",
    "\n",
    "# Compute feature importance\n",
    "feature_importance = compute_feature_importance_gat(gat_model, data, data.y)\n",
    "\n",
    "print(\"Feature Importance:\", feature_importance)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:56.586341300Z",
     "start_time": "2023-11-14T08:04:56.559131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(feature_importance))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:57.054878800Z",
     "start_time": "2023-11-14T08:04:57.038416500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 15])\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:01:52.293875300Z",
     "start_time": "2023-11-14T08:01:52.278989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_19068\\1474834421.py\", line 48, in <module>\n",
      "    explainer = shap.DeepExplainer(model_predict, loader)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py\", line 84, in __init__\n",
      "    self.explainer = TFDeep(model, data, session, learning_phase_flags)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 110, in __init__\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\tf_utils.py\", line 69, in _get_model_inputs\n",
      "AssertionError: <class 'function'> is not currently a supported model type!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim as optim\n",
    "import shap\n",
    "\n",
    "# Define a simple PyTorch GNN model for graph regression with GCNConv\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create a dummy graph dataset\n",
    "# Note: In a real-world scenario, you would load your dataset using PyG's available datasets.\n",
    "x = torch.randn((5, 1))  # Node features (5 nodes, 1 feature per node)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 3, 4], [1, 0, 2, 1, 3, 2, 4, 3]], dtype=torch.long)\n",
    "y = torch.randn((5, 1))  # Target values (regression task)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "\n",
    "# Create a simple GNN model with GCNConv\n",
    "model = SimpleGNN(in_feats=1, hidden_size=16, out_feats=1)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Define a function for the GNN model\n",
    "def model_predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "# Wrap the model_predict function with a shap.DeepExplainer\n",
    "explainer = shap.DeepExplainer(model_predict, loader)\n",
    "\n",
    "# Get Shapley values for a specific data point\n",
    "shap_values = explainer.shap_values(data)\n",
    "\n",
    "# Print Shapley values for each feature\n",
    "print(\"Shapley Values:\", shap_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T06:10:52.811355900Z",
     "start_time": "2023-12-07T06:10:52.749280700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\3908104130.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPlanetoid\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExplainer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mGNNExplainer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGCNConv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 10\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "\n",
    "path = 'feature_importance.png'\n",
    "explanation.visualize_feature_importance(path, top_k=10)\n",
    "print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "path = 'subgraph.pdf'\n",
    "explanation.visualize_graph(path)\n",
    "print(f\"Subgraph visualization plot has been saved to '{path}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T03:38:35.525831900Z",
     "start_time": "2023-12-20T03:38:31.357576100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\52961935.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTUDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T11:05:47.687249Z",
     "start_time": "2023-12-20T11:05:47.457318700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2101667002.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_15604\\2101667002.py\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\u001B[0m\n\u001B[1;37m                                                                      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\n",
    "    {'gnnConv1': ('GATConv', 1, 1), 'gnnConv2': ('LEConv', 13, 6), ... 'KendalTau': {0}},\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4851}},\n",
    "    {'gnnConv1': ('SGConv', 4, 4), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4852}},\n",
    "    {'gnnConv1': ('LEConv', 6, 6), 'gnnConv2': ('GCNConv', 7, 0), ... 'KendalTau': {0.2727}}\n",
    "]\n",
    "\n",
    "max_kendaltau_entry = max(data, key=lambda x: x['KendalTau'].pop())  # Pop the only element from the set\n",
    "\n",
    "print(\"Entry with the highest KendalTau:\")\n",
    "print(max_kendaltau_entry)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T21:49:04.345165800Z",
     "start_time": "2023-12-21T21:49:04.256696400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parts 128\n",
      "partition Partition(rowptr=tensor([    0,    32,    49,  ..., 10552, 10553, 10556]), col=tensor([   2,    9,   10,  ..., 2694, 2700, 2702]), partptr=tensor([   0,   21,   42,   63,   84,  106,  127,  148,  169,  190,  212,  233,\n",
      "         254,  275,  296,  317,  338,  359,  381,  402,  423,  444,  465,  486,\n",
      "         507,  528,  549,  570,  592,  613,  634,  656,  678,  699,  720,  741,\n",
      "         763,  784,  805,  826,  847,  868,  889,  910,  932,  954,  975,  996,\n",
      "        1017, 1038, 1059, 1080, 1101, 1122, 1143, 1164, 1185, 1206, 1227, 1248,\n",
      "        1269, 1290, 1311, 1332, 1353, 1375, 1396, 1417, 1438, 1459, 1481, 1502,\n",
      "        1523, 1544, 1565, 1586, 1607, 1628, 1650, 1671, 1693, 1714, 1735, 1756,\n",
      "        1777, 1798, 1819, 1840, 1862, 1883, 1904, 1926, 1947, 1968, 1989, 2010,\n",
      "        2031, 2052, 2073, 2094, 2116, 2137, 2158, 2180, 2201, 2222, 2243, 2264,\n",
      "        2285, 2306, 2327, 2349, 2370, 2391, 2412, 2433, 2454, 2475, 2496, 2517,\n",
      "        2539, 2560, 2581, 2602, 2623, 2644, 2665, 2687, 2708]), node_perm=tensor([2045, 1624,  126,  ..., 2146,  785, 2486]), edge_perm=tensor([ 8438,  8468,  8451,  ..., 10074, 10075, 10076]))\n",
      "the size of dataloader is  4\n",
      "Epoch 1: Loss: 1.958315670490265\n",
      "Epoch 2: Loss: 1.947250485420227\n",
      "Epoch 3: Loss: 1.9348034858703613\n",
      "Epoch 4: Loss: 1.9225845038890839\n",
      "Epoch 5: Loss: 1.915209710597992\n",
      "Epoch 6: Loss: 1.9022725522518158\n",
      "Epoch 7: Loss: 1.8942180275917053\n",
      "Epoch 8: Loss: 1.8818663656711578\n",
      "Epoch 9: Loss: 1.8729640543460846\n",
      "Epoch 10: Loss: 1.8535686433315277\n",
      "Epoch 11: Loss: 1.8402702808380127\n",
      "Epoch 12: Loss: 1.8281974792480469\n",
      "Epoch 13: Loss: 1.8170470595359802\n",
      "Epoch 14: Loss: 1.8008756339550018\n",
      "Epoch 15: Loss: 1.7821082770824432\n",
      "Epoch 16: Loss: 1.7675354182720184\n",
      "Epoch 17: Loss: 1.7482958734035492\n",
      "Epoch 18: Loss: 1.7310902774333954\n",
      "Epoch 19: Loss: 1.713609129190445\n",
      "Epoch 20: Loss: 1.69693723320961\n",
      "Epoch 21: Loss: 1.6845935583114624\n",
      "Epoch 22: Loss: 1.6691494286060333\n",
      "Epoch 23: Loss: 1.6539480090141296\n",
      "Epoch 24: Loss: 1.6287320852279663\n",
      "Epoch 25: Loss: 1.609798938035965\n",
      "Epoch 26: Loss: 1.5931921899318695\n",
      "Epoch 27: Loss: 1.5796821117401123\n",
      "Epoch 28: Loss: 1.5654276907444\n",
      "Epoch 29: Loss: 1.5574167370796204\n",
      "Epoch 30: Loss: 1.5282230079174042\n",
      "Epoch 31: Loss: 1.5206915736198425\n",
      "Epoch 32: Loss: 1.5055892169475555\n",
      "Epoch 33: Loss: 1.4913874566555023\n",
      "Epoch 34: Loss: 1.4820320010185242\n",
      "Epoch 35: Loss: 1.4684368073940277\n",
      "Epoch 36: Loss: 1.4505507051944733\n",
      "Epoch 37: Loss: 1.4425367414951324\n",
      "Epoch 38: Loss: 1.419445514678955\n",
      "Epoch 39: Loss: 1.3928873538970947\n",
      "Epoch 40: Loss: 1.3882178664207458\n",
      "Epoch 41: Loss: 1.3727969229221344\n",
      "Epoch 42: Loss: 1.3547099232673645\n",
      "Epoch 43: Loss: 1.349112629890442\n",
      "Epoch 44: Loss: 1.3317310214042664\n",
      "Epoch 45: Loss: 1.3200447261333466\n",
      "Epoch 46: Loss: 1.3149255812168121\n",
      "Epoch 47: Loss: 1.2955364286899567\n",
      "Epoch 48: Loss: 1.3025719821453094\n",
      "Epoch 49: Loss: 1.270460993051529\n",
      "Epoch 50: Loss: 1.2630077302455902\n",
      "Epoch 51: Loss: 1.2575254440307617\n",
      "Epoch 52: Loss: 1.2433800995349884\n",
      "Epoch 53: Loss: 1.2207054495811462\n",
      "Epoch 54: Loss: 1.212805598974228\n",
      "Epoch 55: Loss: 1.1963180303573608\n",
      "Epoch 56: Loss: 1.1958113014698029\n",
      "Epoch 57: Loss: 1.1817238926887512\n",
      "Epoch 58: Loss: 1.1821191310882568\n",
      "Epoch 59: Loss: 1.1635279059410095\n",
      "Epoch 60: Loss: 1.1464264392852783\n",
      "Epoch 61: Loss: 1.1233911216259003\n",
      "Epoch 62: Loss: 1.1180004179477692\n",
      "Epoch 63: Loss: 1.1155335307121277\n",
      "Epoch 64: Loss: 1.1019226610660553\n",
      "Epoch 65: Loss: 1.0920244753360748\n",
      "Epoch 66: Loss: 1.0920917689800262\n",
      "Epoch 67: Loss: 1.066989153623581\n",
      "Epoch 68: Loss: 1.0621686577796936\n",
      "Epoch 69: Loss: 1.046364739537239\n",
      "Epoch 70: Loss: 1.044935792684555\n",
      "Epoch 71: Loss: 1.036781907081604\n",
      "Epoch 72: Loss: 1.0430030226707458\n",
      "Epoch 73: Loss: 1.026911273598671\n",
      "Epoch 74: Loss: 1.0155656039714813\n",
      "Epoch 75: Loss: 1.0182998478412628\n",
      "Epoch 76: Loss: 1.0086238235235214\n",
      "Epoch 77: Loss: 1.0010073482990265\n",
      "Epoch 78: Loss: 0.977023646235466\n",
      "Epoch 79: Loss: 0.9785186201334\n",
      "Epoch 80: Loss: 0.975315272808075\n",
      "Epoch 81: Loss: 0.9684127569198608\n",
      "Epoch 82: Loss: 0.9438813626766205\n",
      "Epoch 83: Loss: 0.9559429436922073\n",
      "Epoch 84: Loss: 0.9375476539134979\n",
      "Epoch 85: Loss: 0.9355266988277435\n",
      "Epoch 86: Loss: 0.9493216276168823\n",
      "Epoch 87: Loss: 0.9289489984512329\n",
      "Epoch 88: Loss: 0.9015041440725327\n",
      "Epoch 89: Loss: 0.9024430960416794\n",
      "Epoch 90: Loss: 0.9242920279502869\n",
      "Epoch 91: Loss: 0.8945427685976028\n",
      "Epoch 92: Loss: 0.8735596090555191\n",
      "Epoch 93: Loss: 0.8905325084924698\n",
      "Epoch 94: Loss: 0.873334214091301\n",
      "Epoch 95: Loss: 0.8745541721582413\n",
      "Epoch 96: Loss: 0.8735217154026031\n",
      "Epoch 97: Loss: 0.8606300950050354\n",
      "Epoch 98: Loss: 0.8586615324020386\n",
      "Epoch 99: Loss: 0.8570059537887573\n",
      "Epoch 100: Loss: 0.8530875593423843\n",
      "Epoch 101: Loss: 0.8199320137500763\n",
      "Epoch 102: Loss: 0.8438942283391953\n",
      "Epoch 103: Loss: 0.8339134901762009\n",
      "Epoch 104: Loss: 0.8335314393043518\n",
      "Epoch 105: Loss: 0.8086636811494827\n",
      "Epoch 106: Loss: 0.8119838237762451\n",
      "Epoch 107: Loss: 0.7980286628007889\n",
      "Epoch 108: Loss: 0.7940699309110641\n",
      "Epoch 109: Loss: 0.7989609390497208\n",
      "Epoch 110: Loss: 0.7891949564218521\n",
      "Epoch 111: Loss: 0.7759290635585785\n",
      "Epoch 112: Loss: 0.7768852114677429\n",
      "Epoch 113: Loss: 0.7669669985771179\n",
      "Epoch 114: Loss: 0.7577390223741531\n",
      "Epoch 115: Loss: 0.7775017321109772\n",
      "Epoch 116: Loss: 0.7544242590665817\n",
      "Epoch 117: Loss: 0.7513516545295715\n",
      "Epoch 118: Loss: 0.753930851817131\n",
      "Epoch 119: Loss: 0.7348975837230682\n",
      "Epoch 120: Loss: 0.7450761944055557\n",
      "Epoch 121: Loss: 0.7415353655815125\n",
      "Epoch 122: Loss: 0.7374009191989899\n",
      "Epoch 123: Loss: 0.7335322797298431\n",
      "Epoch 124: Loss: 0.7481644451618195\n",
      "Epoch 125: Loss: 0.7041512131690979\n",
      "Epoch 126: Loss: 0.7077635675668716\n",
      "Epoch 127: Loss: 0.7216883152723312\n",
      "Epoch 128: Loss: 0.6990212500095367\n",
      "Epoch 129: Loss: 0.6989946663379669\n",
      "Epoch 130: Loss: 0.7133537083864212\n",
      "Epoch 131: Loss: 0.7004373371601105\n",
      "Epoch 132: Loss: 0.6937141567468643\n",
      "Epoch 133: Loss: 0.6858424842357635\n",
      "Epoch 134: Loss: 0.6969285756349564\n",
      "Epoch 135: Loss: 0.6885371804237366\n",
      "Epoch 136: Loss: 0.6914992332458496\n",
      "Epoch 137: Loss: 0.7001649290323257\n",
      "Epoch 138: Loss: 0.6761792302131653\n",
      "Epoch 139: Loss: 0.683534786105156\n",
      "Epoch 140: Loss: 0.661896824836731\n",
      "Epoch 141: Loss: 0.6560716181993484\n",
      "Epoch 142: Loss: 0.6582814157009125\n",
      "Epoch 143: Loss: 0.6608859151601791\n",
      "Epoch 144: Loss: 0.6447287946939468\n",
      "Epoch 145: Loss: 0.6512546837329865\n",
      "Epoch 146: Loss: 0.6608547270298004\n",
      "Epoch 147: Loss: 0.6568313241004944\n",
      "Epoch 148: Loss: 0.6493037790060043\n",
      "Epoch 149: Loss: 0.6211657226085663\n",
      "Epoch 150: Loss: 0.6343398690223694\n",
      "Epoch 151: Loss: 0.642907127737999\n",
      "Epoch 152: Loss: 0.6268937885761261\n",
      "Epoch 153: Loss: 0.6280461996793747\n",
      "Epoch 154: Loss: 0.6178964376449585\n",
      "Epoch 155: Loss: 0.6287286132574081\n",
      "Epoch 156: Loss: 0.6190201789140701\n",
      "Epoch 157: Loss: 0.6148691028356552\n",
      "Epoch 158: Loss: 0.627339094877243\n",
      "Epoch 159: Loss: 0.6155747473239899\n",
      "Epoch 160: Loss: 0.6090388149023056\n",
      "Epoch 161: Loss: 0.618185356259346\n",
      "Epoch 162: Loss: 0.6145650446414948\n",
      "Epoch 163: Loss: 0.5952434092760086\n",
      "Epoch 164: Loss: 0.600506529211998\n",
      "Epoch 165: Loss: 0.5992493331432343\n",
      "Epoch 166: Loss: 0.5813777297735214\n",
      "Epoch 167: Loss: 0.6015961766242981\n",
      "Epoch 168: Loss: 0.5811042636632919\n",
      "Epoch 169: Loss: 0.5922313332557678\n",
      "Epoch 170: Loss: 0.5838097631931305\n",
      "Epoch 171: Loss: 0.5749604403972626\n",
      "Epoch 172: Loss: 0.5951420366764069\n",
      "Epoch 173: Loss: 0.5746815800666809\n",
      "Epoch 174: Loss: 0.5930315554141998\n",
      "Epoch 175: Loss: 0.5589471757411957\n",
      "Epoch 176: Loss: 0.5783902108669281\n",
      "Epoch 177: Loss: 0.5661034286022186\n",
      "Epoch 178: Loss: 0.5601937770843506\n",
      "Epoch 179: Loss: 0.5656999573111534\n",
      "Epoch 180: Loss: 0.5671752542257309\n",
      "Epoch 181: Loss: 0.553933635354042\n",
      "Epoch 182: Loss: 0.5477162823081017\n",
      "Epoch 183: Loss: 0.5689652562141418\n",
      "Epoch 184: Loss: 0.5764508694410324\n",
      "Epoch 185: Loss: 0.5548906624317169\n",
      "Epoch 186: Loss: 0.5528040826320648\n",
      "Epoch 187: Loss: 0.5537468641996384\n",
      "Epoch 188: Loss: 0.5376491844654083\n",
      "Epoch 189: Loss: 0.5410222262144089\n",
      "Epoch 190: Loss: 0.5537997111678123\n",
      "Epoch 191: Loss: 0.5406991615891457\n",
      "Epoch 192: Loss: 0.5461208522319794\n",
      "Epoch 193: Loss: 0.5518131926655769\n",
      "Epoch 194: Loss: 0.5332967787981033\n",
      "Epoch 195: Loss: 0.516054630279541\n",
      "Epoch 196: Loss: 0.5361490994691849\n",
      "Epoch 197: Loss: 0.5233540236949921\n",
      "Epoch 198: Loss: 0.5340309888124466\n",
      "Epoch 199: Loss: 0.527574211359024\n",
      "Epoch 200: Loss: 0.5376299247145653\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader,NodeLoader, ClusterLoader, ClusterData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Coauthor\n",
    "from torch_geometric.sampler import BaseSampler\n",
    "# Assuming dataset is a list of data objects, each representing a graph or a subgraph\n",
    "# dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "dataset=Planetoid(root=\"data/test\",name=\"Cora\")\n",
    "data=dataset[0]\n",
    "\n",
    "cluster_data = ClusterData(data, num_parts=128)\n",
    "print(\"Num parts\",cluster_data.num_parts)\n",
    "print(\"partition\",cluster_data.partition)\n",
    "loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)\n",
    "print(\"the size of dataloader is \", len(loader))\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index = batch.x, batch.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00035, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:39:38.439328800Z",
     "start_time": "2024-02-17T12:38:51.448755100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        correct += int(pred.eq(data.y).sum().item())\n",
    "        total += data.x.shape[0]\n",
    "    return correct / total, total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:51.859192800Z",
     "start_time": "2024-02-17T12:20:51.842193900Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_accuracy,total = evaluate(loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:52.311906900Z",
     "start_time": "2024-02-17T12:20:52.261265400Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9523633677991138, 2708)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy, total\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:52.968052400Z",
     "start_time": "2024-02-17T12:20:52.938969500Z"
    }
   },
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dataset[0]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T14:16:14.785201200Z",
     "start_time": "2024-02-17T14:16:14.770197400Z"
    }
   },
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset2 = Planetoid(root='/tmp/Cora', name='Cora')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T04:49:17.346086600Z",
     "start_time": "2024-02-17T04:49:17.317582500Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data2=dataset2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T04:49:18.370197300Z",
     "start_time": "2024-02-17T04:49:18.352195700Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T04:49:22.067658400Z",
     "start_time": "2024-02-17T04:49:22.054658700Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
