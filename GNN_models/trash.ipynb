{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.63994, Train Acc: 0.67166, Test Acc: 0.77477\n",
      "Epoch: 002, Loss: 0.60602, Train Acc: 0.72555, Test Acc: 0.75676\n",
      "Epoch: 003, Loss: 0.59269, Train Acc: 0.73553, Test Acc: 0.78378\n",
      "Epoch: 004, Loss: 0.57202, Train Acc: 0.73353, Test Acc: 0.75676\n",
      "Epoch: 005, Loss: 0.56646, Train Acc: 0.73553, Test Acc: 0.72973\n",
      "Epoch: 006, Loss: 0.56377, Train Acc: 0.74451, Test Acc: 0.76577\n",
      "Epoch: 007, Loss: 0.53971, Train Acc: 0.74950, Test Acc: 0.77477\n",
      "Epoch: 008, Loss: 0.54838, Train Acc: 0.74750, Test Acc: 0.76577\n",
      "Epoch: 009, Loss: 0.54948, Train Acc: 0.75349, Test Acc: 0.75676\n",
      "Epoch: 010, Loss: 0.53813, Train Acc: 0.75349, Test Acc: 0.76577\n",
      "Epoch: 011, Loss: 0.53843, Train Acc: 0.75150, Test Acc: 0.76577\n",
      "Epoch: 012, Loss: 0.53526, Train Acc: 0.75649, Test Acc: 0.74775\n",
      "Epoch: 013, Loss: 0.53497, Train Acc: 0.76347, Test Acc: 0.76577\n",
      "Epoch: 014, Loss: 0.52893, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 015, Loss: 0.52412, Train Acc: 0.75848, Test Acc: 0.74775\n",
      "Epoch: 016, Loss: 0.52143, Train Acc: 0.75749, Test Acc: 0.73874\n",
      "Epoch: 017, Loss: 0.51217, Train Acc: 0.76248, Test Acc: 0.76577\n",
      "Epoch: 018, Loss: 0.50869, Train Acc: 0.76447, Test Acc: 0.76577\n",
      "Epoch: 019, Loss: 0.50850, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 020, Loss: 0.50986, Train Acc: 0.76946, Test Acc: 0.73874\n",
      "Epoch: 021, Loss: 0.50446, Train Acc: 0.76647, Test Acc: 0.74775\n",
      "Epoch: 022, Loss: 0.50306, Train Acc: 0.77445, Test Acc: 0.74775\n",
      "Epoch: 023, Loss: 0.50349, Train Acc: 0.77046, Test Acc: 0.73874\n",
      "Epoch: 024, Loss: 0.50693, Train Acc: 0.77246, Test Acc: 0.75676\n",
      "Epoch: 025, Loss: 0.50562, Train Acc: 0.76846, Test Acc: 0.76577\n",
      "Epoch: 026, Loss: 0.50851, Train Acc: 0.77645, Test Acc: 0.75676\n",
      "Epoch: 027, Loss: 0.51166, Train Acc: 0.77445, Test Acc: 0.75676\n",
      "Epoch: 028, Loss: 0.50159, Train Acc: 0.77944, Test Acc: 0.75676\n",
      "Epoch: 029, Loss: 0.49367, Train Acc: 0.78044, Test Acc: 0.74775\n",
      "Epoch: 030, Loss: 0.49450, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 031, Loss: 0.48509, Train Acc: 0.78842, Test Acc: 0.77477\n",
      "Epoch: 032, Loss: 0.49752, Train Acc: 0.78842, Test Acc: 0.75676\n",
      "Epoch: 033, Loss: 0.47788, Train Acc: 0.79242, Test Acc: 0.75676\n",
      "Epoch: 034, Loss: 0.47482, Train Acc: 0.78842, Test Acc: 0.74775\n",
      "Epoch: 035, Loss: 0.48345, Train Acc: 0.79042, Test Acc: 0.75676\n",
      "Epoch: 036, Loss: 0.47648, Train Acc: 0.78643, Test Acc: 0.76577\n",
      "Epoch: 037, Loss: 0.47779, Train Acc: 0.79341, Test Acc: 0.77477\n",
      "Epoch: 038, Loss: 0.47345, Train Acc: 0.79042, Test Acc: 0.77477\n",
      "Epoch: 039, Loss: 0.47719, Train Acc: 0.79142, Test Acc: 0.75676\n",
      "Epoch: 040, Loss: 0.46943, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 041, Loss: 0.45909, Train Acc: 0.79242, Test Acc: 0.79279\n",
      "Epoch: 042, Loss: 0.45949, Train Acc: 0.78942, Test Acc: 0.75676\n",
      "Epoch: 043, Loss: 0.47938, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 044, Loss: 0.45838, Train Acc: 0.80040, Test Acc: 0.77477\n",
      "Epoch: 045, Loss: 0.44669, Train Acc: 0.80140, Test Acc: 0.78378\n",
      "Epoch: 046, Loss: 0.45047, Train Acc: 0.79840, Test Acc: 0.78378\n",
      "Epoch: 047, Loss: 0.46048, Train Acc: 0.80040, Test Acc: 0.75676\n",
      "Epoch: 048, Loss: 0.45371, Train Acc: 0.79840, Test Acc: 0.75676\n",
      "Epoch: 049, Loss: 0.45315, Train Acc: 0.79341, Test Acc: 0.75676\n",
      "Epoch: 050, Loss: 0.44780, Train Acc: 0.79341, Test Acc: 0.76577\n",
      "Epoch: 051, Loss: 0.44573, Train Acc: 0.79541, Test Acc: 0.76577\n",
      "Epoch: 052, Loss: 0.43719, Train Acc: 0.78543, Test Acc: 0.74775\n",
      "Epoch: 053, Loss: 0.45039, Train Acc: 0.80838, Test Acc: 0.76577\n",
      "Epoch: 054, Loss: 0.44865, Train Acc: 0.80339, Test Acc: 0.78378\n",
      "Epoch: 055, Loss: 0.43105, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 056, Loss: 0.43284, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 057, Loss: 0.42553, Train Acc: 0.81238, Test Acc: 0.77477\n",
      "Epoch: 058, Loss: 0.42388, Train Acc: 0.80140, Test Acc: 0.76577\n",
      "Epoch: 059, Loss: 0.42252, Train Acc: 0.80240, Test Acc: 0.74775\n",
      "Epoch: 060, Loss: 0.42339, Train Acc: 0.81337, Test Acc: 0.76577\n",
      "Epoch: 061, Loss: 0.41549, Train Acc: 0.81138, Test Acc: 0.76577\n",
      "Epoch: 062, Loss: 0.41829, Train Acc: 0.81038, Test Acc: 0.75676\n",
      "Epoch: 063, Loss: 0.41101, Train Acc: 0.81038, Test Acc: 0.74775\n",
      "Epoch: 064, Loss: 0.41326, Train Acc: 0.81537, Test Acc: 0.73874\n",
      "Epoch: 065, Loss: 0.40395, Train Acc: 0.80439, Test Acc: 0.75676\n",
      "Epoch: 066, Loss: 0.40527, Train Acc: 0.81936, Test Acc: 0.76577\n",
      "Epoch: 067, Loss: 0.40050, Train Acc: 0.82435, Test Acc: 0.76577\n",
      "Epoch: 068, Loss: 0.39777, Train Acc: 0.82036, Test Acc: 0.76577\n",
      "Epoch: 069, Loss: 0.39965, Train Acc: 0.82635, Test Acc: 0.73874\n",
      "Epoch: 070, Loss: 0.38398, Train Acc: 0.83034, Test Acc: 0.73874\n",
      "Epoch: 071, Loss: 0.39805, Train Acc: 0.82236, Test Acc: 0.74775\n",
      "Epoch: 072, Loss: 0.39025, Train Acc: 0.81836, Test Acc: 0.75676\n",
      "Epoch: 073, Loss: 0.41702, Train Acc: 0.80439, Test Acc: 0.77477\n",
      "Epoch: 074, Loss: 0.41968, Train Acc: 0.81537, Test Acc: 0.76577\n",
      "Epoch: 075, Loss: 0.39959, Train Acc: 0.82735, Test Acc: 0.76577\n",
      "Epoch: 076, Loss: 0.37986, Train Acc: 0.82735, Test Acc: 0.73874\n",
      "Epoch: 077, Loss: 0.38195, Train Acc: 0.83433, Test Acc: 0.76577\n",
      "Epoch: 078, Loss: 0.36943, Train Acc: 0.83733, Test Acc: 0.74775\n",
      "Epoch: 079, Loss: 0.35535, Train Acc: 0.82535, Test Acc: 0.72973\n",
      "Epoch: 080, Loss: 0.37084, Train Acc: 0.82136, Test Acc: 0.72072\n",
      "Epoch: 081, Loss: 0.37048, Train Acc: 0.83034, Test Acc: 0.72973\n",
      "Epoch: 082, Loss: 0.36895, Train Acc: 0.82735, Test Acc: 0.69369\n",
      "Epoch: 083, Loss: 0.37141, Train Acc: 0.83533, Test Acc: 0.76577\n",
      "Epoch: 084, Loss: 0.35657, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 085, Loss: 0.35072, Train Acc: 0.82934, Test Acc: 0.70270\n",
      "Epoch: 086, Loss: 0.35426, Train Acc: 0.84331, Test Acc: 0.72973\n",
      "Epoch: 087, Loss: 0.34678, Train Acc: 0.85130, Test Acc: 0.70270\n",
      "Epoch: 088, Loss: 0.33941, Train Acc: 0.84531, Test Acc: 0.72973\n",
      "Epoch: 089, Loss: 0.33547, Train Acc: 0.85329, Test Acc: 0.73874\n",
      "Epoch: 090, Loss: 0.32926, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 091, Loss: 0.31867, Train Acc: 0.86327, Test Acc: 0.73874\n",
      "Epoch: 092, Loss: 0.32957, Train Acc: 0.86128, Test Acc: 0.70270\n",
      "Epoch: 093, Loss: 0.31665, Train Acc: 0.85030, Test Acc: 0.74775\n",
      "Epoch: 094, Loss: 0.31365, Train Acc: 0.85429, Test Acc: 0.76577\n",
      "Epoch: 095, Loss: 0.32666, Train Acc: 0.85928, Test Acc: 0.74775\n",
      "Epoch: 096, Loss: 0.32321, Train Acc: 0.85130, Test Acc: 0.72973\n",
      "Epoch: 097, Loss: 0.32056, Train Acc: 0.87425, Test Acc: 0.77477\n",
      "Epoch: 098, Loss: 0.30211, Train Acc: 0.86627, Test Acc: 0.73874\n",
      "Epoch: 099, Loss: 0.31116, Train Acc: 0.85928, Test Acc: 0.78378\n",
      "Epoch: 100, Loss: 0.30914, Train Acc: 0.87625, Test Acc: 0.76577\n",
      "Epoch: 101, Loss: 0.31156, Train Acc: 0.87625, Test Acc: 0.75676\n",
      "Epoch: 102, Loss: 0.29537, Train Acc: 0.87824, Test Acc: 0.72072\n",
      "Epoch: 103, Loss: 0.30042, Train Acc: 0.88723, Test Acc: 0.74775\n",
      "Epoch: 104, Loss: 0.28999, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 105, Loss: 0.28154, Train Acc: 0.86427, Test Acc: 0.74775\n",
      "Epoch: 106, Loss: 0.27667, Train Acc: 0.88523, Test Acc: 0.77477\n",
      "Epoch: 107, Loss: 0.26873, Train Acc: 0.88224, Test Acc: 0.73874\n",
      "Epoch: 108, Loss: 0.28136, Train Acc: 0.89022, Test Acc: 0.77477\n",
      "Epoch: 109, Loss: 0.27982, Train Acc: 0.89421, Test Acc: 0.76577\n",
      "Epoch: 110, Loss: 0.26023, Train Acc: 0.90519, Test Acc: 0.72072\n",
      "Epoch: 111, Loss: 0.26328, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 112, Loss: 0.25424, Train Acc: 0.89621, Test Acc: 0.73874\n",
      "Epoch: 113, Loss: 0.25966, Train Acc: 0.89421, Test Acc: 0.78378\n",
      "Epoch: 114, Loss: 0.24187, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 115, Loss: 0.25082, Train Acc: 0.89820, Test Acc: 0.77477\n",
      "Epoch: 116, Loss: 0.28134, Train Acc: 0.91317, Test Acc: 0.73874\n",
      "Epoch: 117, Loss: 0.24313, Train Acc: 0.90918, Test Acc: 0.74775\n",
      "Epoch: 118, Loss: 0.25145, Train Acc: 0.90918, Test Acc: 0.72973\n",
      "Epoch: 119, Loss: 0.26201, Train Acc: 0.90319, Test Acc: 0.72973\n",
      "Epoch: 120, Loss: 0.23378, Train Acc: 0.91617, Test Acc: 0.69369\n",
      "Epoch: 121, Loss: 0.24473, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 122, Loss: 0.23172, Train Acc: 0.91517, Test Acc: 0.75676\n",
      "Epoch: 123, Loss: 0.23335, Train Acc: 0.92016, Test Acc: 0.75676\n",
      "Epoch: 124, Loss: 0.20975, Train Acc: 0.92216, Test Acc: 0.73874\n",
      "Epoch: 125, Loss: 0.20874, Train Acc: 0.92515, Test Acc: 0.76577\n",
      "Epoch: 126, Loss: 0.20907, Train Acc: 0.93014, Test Acc: 0.77477\n",
      "Epoch: 127, Loss: 0.20823, Train Acc: 0.92515, Test Acc: 0.70270\n",
      "Epoch: 128, Loss: 0.20784, Train Acc: 0.93513, Test Acc: 0.73874\n",
      "Epoch: 129, Loss: 0.20660, Train Acc: 0.93812, Test Acc: 0.74775\n",
      "Epoch: 130, Loss: 0.22070, Train Acc: 0.92914, Test Acc: 0.71171\n",
      "Epoch: 131, Loss: 0.22617, Train Acc: 0.93413, Test Acc: 0.78378\n",
      "Epoch: 132, Loss: 0.20191, Train Acc: 0.93513, Test Acc: 0.71171\n",
      "Epoch: 133, Loss: 0.20896, Train Acc: 0.93613, Test Acc: 0.76577\n",
      "Epoch: 134, Loss: 0.22466, Train Acc: 0.92216, Test Acc: 0.72973\n",
      "Epoch: 135, Loss: 0.24093, Train Acc: 0.93613, Test Acc: 0.66667\n",
      "Epoch: 136, Loss: 0.21365, Train Acc: 0.93812, Test Acc: 0.70270\n",
      "Epoch: 137, Loss: 0.22885, Train Acc: 0.92016, Test Acc: 0.73874\n",
      "Epoch: 138, Loss: 0.23151, Train Acc: 0.92016, Test Acc: 0.69369\n",
      "Epoch: 139, Loss: 0.23535, Train Acc: 0.92715, Test Acc: 0.72973\n",
      "Epoch: 140, Loss: 0.20304, Train Acc: 0.93014, Test Acc: 0.72072\n",
      "Epoch: 141, Loss: 0.21420, Train Acc: 0.93313, Test Acc: 0.73874\n",
      "Epoch: 142, Loss: 0.19753, Train Acc: 0.93413, Test Acc: 0.73874\n",
      "Epoch: 143, Loss: 0.20483, Train Acc: 0.94212, Test Acc: 0.72973\n",
      "Epoch: 144, Loss: 0.19896, Train Acc: 0.95609, Test Acc: 0.72072\n",
      "Epoch: 145, Loss: 0.17311, Train Acc: 0.96307, Test Acc: 0.68468\n",
      "Epoch: 146, Loss: 0.15830, Train Acc: 0.94910, Test Acc: 0.69369\n",
      "Epoch: 147, Loss: 0.18470, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 148, Loss: 0.18968, Train Acc: 0.95210, Test Acc: 0.70270\n",
      "Epoch: 149, Loss: 0.14472, Train Acc: 0.94611, Test Acc: 0.72072\n",
      "Epoch: 150, Loss: 0.16253, Train Acc: 0.96108, Test Acc: 0.74775\n",
      "Epoch: 151, Loss: 0.16207, Train Acc: 0.97206, Test Acc: 0.74775\n",
      "Epoch: 152, Loss: 0.15024, Train Acc: 0.95210, Test Acc: 0.72973\n",
      "Epoch: 153, Loss: 0.17165, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 154, Loss: 0.15945, Train Acc: 0.95908, Test Acc: 0.72072\n",
      "Epoch: 155, Loss: 0.18727, Train Acc: 0.94511, Test Acc: 0.71171\n",
      "Epoch: 156, Loss: 0.15213, Train Acc: 0.95409, Test Acc: 0.72973\n",
      "Epoch: 157, Loss: 0.15487, Train Acc: 0.93214, Test Acc: 0.68468\n",
      "Epoch: 158, Loss: 0.19028, Train Acc: 0.93912, Test Acc: 0.70270\n",
      "Epoch: 159, Loss: 0.15470, Train Acc: 0.93812, Test Acc: 0.66667\n",
      "Epoch: 160, Loss: 0.16697, Train Acc: 0.92814, Test Acc: 0.64865\n",
      "Epoch: 161, Loss: 0.18450, Train Acc: 0.94611, Test Acc: 0.67568\n",
      "Epoch: 162, Loss: 0.21105, Train Acc: 0.91218, Test Acc: 0.63063\n",
      "Epoch: 163, Loss: 0.19326, Train Acc: 0.90120, Test Acc: 0.71171\n",
      "Epoch: 164, Loss: 0.20128, Train Acc: 0.92715, Test Acc: 0.67568\n",
      "Epoch: 165, Loss: 0.20114, Train Acc: 0.92515, Test Acc: 0.66667\n",
      "Epoch: 166, Loss: 0.17511, Train Acc: 0.94611, Test Acc: 0.74775\n",
      "Epoch: 167, Loss: 0.15736, Train Acc: 0.93812, Test Acc: 0.73874\n",
      "Epoch: 168, Loss: 0.19465, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 169, Loss: 0.16225, Train Acc: 0.95210, Test Acc: 0.72072\n",
      "Epoch: 170, Loss: 0.18879, Train Acc: 0.93912, Test Acc: 0.75676\n",
      "Epoch: 171, Loss: 0.19217, Train Acc: 0.91118, Test Acc: 0.72072\n",
      "Epoch: 172, Loss: 0.19118, Train Acc: 0.95309, Test Acc: 0.72072\n",
      "Epoch: 173, Loss: 0.17934, Train Acc: 0.95409, Test Acc: 0.73874\n",
      "Epoch: 174, Loss: 0.15845, Train Acc: 0.95908, Test Acc: 0.72973\n",
      "Epoch: 175, Loss: 0.18280, Train Acc: 0.95110, Test Acc: 0.69369\n",
      "Epoch: 176, Loss: 0.20469, Train Acc: 0.93313, Test Acc: 0.67568\n",
      "Epoch: 177, Loss: 0.22061, Train Acc: 0.96407, Test Acc: 0.67568\n",
      "Epoch: 178, Loss: 0.15292, Train Acc: 0.95609, Test Acc: 0.72973\n",
      "Epoch: 179, Loss: 0.15615, Train Acc: 0.95110, Test Acc: 0.75676\n",
      "Epoch: 180, Loss: 0.15942, Train Acc: 0.96707, Test Acc: 0.76577\n",
      "Epoch: 181, Loss: 0.14494, Train Acc: 0.96208, Test Acc: 0.73874\n",
      "Epoch: 182, Loss: 0.13089, Train Acc: 0.97206, Test Acc: 0.73874\n",
      "Epoch: 183, Loss: 0.12631, Train Acc: 0.96607, Test Acc: 0.73874\n",
      "Epoch: 184, Loss: 0.12990, Train Acc: 0.94910, Test Acc: 0.73874\n",
      "Epoch: 185, Loss: 0.14997, Train Acc: 0.96607, Test Acc: 0.75676\n",
      "Epoch: 186, Loss: 0.11839, Train Acc: 0.97605, Test Acc: 0.74775\n",
      "Epoch: 187, Loss: 0.10730, Train Acc: 0.98104, Test Acc: 0.71171\n",
      "Epoch: 188, Loss: 0.12760, Train Acc: 0.98104, Test Acc: 0.69369\n",
      "Epoch: 189, Loss: 0.10207, Train Acc: 0.98303, Test Acc: 0.75676\n",
      "Epoch: 190, Loss: 0.10017, Train Acc: 0.98204, Test Acc: 0.75676\n",
      "Epoch: 191, Loss: 0.11475, Train Acc: 0.98603, Test Acc: 0.68468\n",
      "Epoch: 192, Loss: 0.10596, Train Acc: 0.98104, Test Acc: 0.70270\n",
      "Epoch: 193, Loss: 0.11342, Train Acc: 0.96507, Test Acc: 0.71171\n",
      "Epoch: 194, Loss: 0.13239, Train Acc: 0.97505, Test Acc: 0.74775\n",
      "Epoch: 195, Loss: 0.10453, Train Acc: 0.97605, Test Acc: 0.72973\n",
      "Epoch: 196, Loss: 0.10578, Train Acc: 0.95309, Test Acc: 0.70270\n",
      "Epoch: 197, Loss: 0.11440, Train Acc: 0.96906, Test Acc: 0.68468\n",
      "Epoch: 198, Loss: 0.13735, Train Acc: 0.95709, Test Acc: 0.74775\n",
      "Epoch: 199, Loss: 0.14729, Train Acc: 0.96507, Test Acc: 0.76577\n",
      "Epoch: 200, Loss: 0.11679, Train Acc: 0.97405, Test Acc: 0.71171\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "\n",
    "path = 'data/TUDataset'\n",
    "dataset = TUDataset(path, name='PROTEINS')\n",
    "dataset = dataset.shuffle()\n",
    "n = len(dataset) // 10\n",
    "test_dataset = dataset[:n]\n",
    "train_dataset = dataset[n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=60)\n",
    "train_loader = DataLoader(train_dataset, batch_size=60)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(dataset.num_features, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, '\n",
    "          f'Test Acc: {test_acc:.5f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  10%|█         | 1/10 [00:00<00:00,  9.78number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|███       | 3/10 [00:00<00:00,  9.25number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|█████     | 5/10 [00:00<00:00,  9.19number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|███████   | 7/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  90%|█████████ | 9/10 [00:00<00:00,  9.16number/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 10/10 [00:01<00:00,  9.19number/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in tqdm(range(1, n+1), desc='Progress', unit='number'):\n",
    "    print(i)\n",
    "    time.sleep(0.1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance: tensor([[ 1.2275e-02, -4.3866e-04, -2.3747e-03,  5.9961e-06, -1.8322e-02,\n",
      "          6.7758e-03, -3.3756e-03,  3.3045e-03, -4.1410e-04, -4.7517e-03,\n",
      "          6.5441e-03,  1.1756e-04,  8.9861e-03, -6.2483e-03, -2.0754e-04],\n",
      "        [ 9.7265e-04, -4.1853e-04,  8.4249e-04, -4.1004e-04, -1.3501e-03,\n",
      "         -2.3933e-04, -1.1280e-03, -7.0684e-04,  3.6807e-04, -7.3343e-04,\n",
      "          6.7297e-04,  1.7970e-05,  6.3480e-04, -1.7746e-04, -7.7775e-04],\n",
      "        [ 7.6379e-04, -4.1705e-04,  5.0388e-03,  9.4725e-03,  1.3427e-03,\n",
      "         -3.9083e-03, -7.4786e-04, -9.5126e-03,  2.7012e-03, -1.3322e-02,\n",
      "         -2.7063e-03, -5.9831e-03, -1.0255e-02,  6.4585e-03, -2.2670e-03],\n",
      "        [ 2.3280e-02, -1.9069e-03,  1.2953e-02, -1.7481e-03, -2.7319e-03,\n",
      "          2.4886e-02, -1.2913e-02,  4.1282e-04, -2.1938e-02,  1.2066e-02,\n",
      "          9.2277e-03,  1.7172e-02,  2.6745e-02, -1.2474e-02, -8.2982e-03],\n",
      "        [ 7.0498e-03, -1.2809e-03, -4.0443e-03,  5.3280e-03, -1.0616e-03,\n",
      "          5.3567e-03, -1.1408e-02,  4.7099e-03,  2.3087e-03, -2.3428e-03,\n",
      "         -3.9236e-03,  3.3648e-03,  1.6152e-02, -1.9746e-03, -3.1419e-03],\n",
      "        [ 2.0292e-03,  1.5952e-04, -1.1887e-03,  5.4479e-04, -5.4863e-04,\n",
      "          1.9303e-03, -1.4612e-03,  2.2287e-03, -1.4091e-03,  4.1199e-04,\n",
      "         -7.8360e-04,  9.2731e-04,  2.3172e-03, -7.5774e-04, -1.5543e-03],\n",
      "        [ 8.9949e-03,  8.8533e-04, -2.1451e-03,  7.0428e-03,  4.7685e-03,\n",
      "          5.8864e-03, -6.4042e-03,  3.9720e-03, -5.3514e-03, -1.2071e-03,\n",
      "         -1.4672e-03,  4.7018e-03,  7.4761e-03, -1.3038e-03, -1.2246e-02],\n",
      "        [-6.6790e-03, -4.5793e-03, -1.3295e-02, -4.9643e-04,  9.9808e-03,\n",
      "         -1.7760e-02,  3.9535e-03, -3.5023e-03,  1.9258e-02, -1.2740e-03,\n",
      "          2.0336e-03, -5.7606e-03, -5.5823e-03, -3.2434e-03,  1.3776e-03],\n",
      "        [-4.9399e-03, -3.0927e-03,  3.5786e-03, -3.8228e-03,  1.2745e-03,\n",
      "         -5.3330e-03, -1.1231e-03, -2.8851e-03,  4.4198e-03,  7.0476e-04,\n",
      "         -2.8511e-03, -1.5016e-03, -4.2478e-03,  4.3503e-03,  5.4290e-03],\n",
      "        [-1.1208e-03, -3.3793e-05,  3.4144e-04, -5.5016e-04,  1.5730e-04,\n",
      "         -6.2237e-04,  6.3486e-04, -3.7564e-04,  4.7872e-04,  3.7283e-04,\n",
      "         -6.9144e-07, -2.4358e-04, -8.4744e-04, -1.0327e-04,  7.2700e-04],\n",
      "        [ 7.5913e-03, -3.6760e-04,  2.5405e-04,  1.4391e-02, -7.0900e-03,\n",
      "          9.2784e-03, -2.1551e-03,  1.4613e-02, -8.1834e-03, -8.6680e-03,\n",
      "         -6.1243e-03, -1.5186e-04,  2.0315e-02, -8.2295e-03, -1.3247e-02],\n",
      "        [-3.6148e-04,  1.8751e-04, -7.9749e-05, -1.9293e-04, -9.9177e-05,\n",
      "         -3.3470e-04,  4.6496e-05,  3.7124e-05,  3.1666e-04,  4.6569e-04,\n",
      "         -2.0883e-04,  3.1561e-05, -4.3384e-04, -5.1910e-05,  5.3931e-04],\n",
      "        [ 5.4399e-03,  3.3108e-03, -5.0663e-03, -1.5073e-04,  1.5731e-03,\n",
      "          1.9585e-03, -1.6530e-03,  2.5281e-03, -2.4772e-03,  1.0186e-03,\n",
      "         -4.4670e-04,  1.4380e-03,  1.3553e-03, -2.0077e-04,  1.7682e-03],\n",
      "        [-3.7684e-04,  1.4193e-03,  2.0130e-03,  8.5876e-04, -2.0750e-03,\n",
      "          4.1714e-03, -2.5274e-04,  3.3690e-03, -3.6807e-03, -1.2927e-03,\n",
      "          5.9008e-06,  5.0497e-04,  2.1869e-03, -2.3429e-03, -2.2337e-03],\n",
      "        [-1.8282e-02,  9.8750e-03,  9.4390e-03, -6.5219e-03, -4.1733e-03,\n",
      "         -9.3553e-03,  1.7525e-02, -2.4783e-03, -9.8174e-03,  1.2298e-02,\n",
      "          1.1746e-02,  1.1692e-03, -1.3515e-02,  7.1292e-03,  1.6060e-02],\n",
      "        [ 1.7786e-03, -1.1181e-03,  9.1284e-05, -3.2955e-04, -1.9153e-03,\n",
      "         -1.1905e-03, -1.4411e-03, -2.0828e-04,  9.0699e-04, -1.7265e-03,\n",
      "         -5.3761e-04,  5.5327e-04,  2.0214e-03, -4.0939e-04, -2.7614e-04],\n",
      "        [ 1.0221e-02, -3.5139e-03, -2.9626e-03,  6.8862e-03, -1.3246e-02,\n",
      "          2.3670e-03, -5.1162e-03,  3.1607e-03, -4.1904e-03, -1.3504e-02,\n",
      "         -1.6872e-03,  2.1691e-03,  2.1273e-02, -2.9301e-03, -1.5891e-02],\n",
      "        [ 5.3006e-03,  1.3953e-03, -1.4417e-03,  1.0278e-03,  2.5639e-05,\n",
      "          2.8371e-03,  3.2918e-04, -9.2075e-04, -1.7274e-03, -5.2760e-04,\n",
      "          1.0792e-03, -1.3019e-04, -1.2930e-04,  7.9166e-05,  2.3511e-04],\n",
      "        [-1.0612e-02, -2.1024e-02,  5.7821e-03, -2.2025e-02,  1.7100e-03,\n",
      "         -3.3466e-02, -1.3780e-02, -1.8313e-02,  2.8295e-02,  7.7237e-03,\n",
      "         -1.3175e-02, -1.3969e-02, -1.6624e-02,  1.2225e-02,  2.6744e-02],\n",
      "        [ 3.8612e-03,  2.0107e-03,  1.6856e-03, -3.5960e-04, -1.3485e-02,\n",
      "          9.4410e-03,  1.2272e-03,  6.6000e-03, -5.0133e-03, -5.0718e-03,\n",
      "          4.1998e-03,  2.1863e-03,  4.9845e-03, -3.0835e-03, -3.4995e-03]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Define a simple GAT-based GNN model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_heads):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_size, hidden_size, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_size * num_heads, output_size, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Function to compute feature importance using gradient\n",
    "def compute_feature_importance_gat(model, data, target):\n",
    "    model.eval()\n",
    "    data.x.requires_grad_(True)\n",
    "\n",
    "    optimizer = optim.SGD([data.x], lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Compute feature importance as the absolute gradient\n",
    "    # feature_importance = torch.abs(data.x.grad)\n",
    "    feature_importance = data.x.grad\n",
    "\n",
    "    return feature_importance\n",
    "\n",
    "# Example usage\n",
    "# Assume your GNN model takes a graph as input, represented using the PyTorch Geometric data structure.\n",
    "# Here, we use a simple example with a random graph.\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "input_size=15\n",
    "output_size=1\n",
    "hidden_size=64\n",
    "# Generate a random graph\n",
    "num_nodes = 20\n",
    "num_edges = 15\n",
    "data = Data(x=torch.randn((num_nodes, input_size), requires_grad=True),\n",
    "            edge_index=torch.randint(0, num_nodes, (2, num_edges), dtype=torch.long),\n",
    "            y=torch.randn((num_nodes, output_size)))\n",
    "\n",
    "# Initialize the GAT-based GNN model\n",
    "gat_model = GATModel(input_size, hidden_size, output_size, num_heads=2)\n",
    "\n",
    "# Compute feature importance\n",
    "feature_importance = compute_feature_importance_gat(gat_model, data, data.y)\n",
    "\n",
    "print(\"Feature Importance:\", feature_importance)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:56.586341300Z",
     "start_time": "2023-11-14T08:04:56.559131Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(feature_importance))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:04:57.054878800Z",
     "start_time": "2023-11-14T08:04:57.038416500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 15])\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T08:01:52.293875300Z",
     "start_time": "2023-11-14T08:01:52.278989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_19068\\1474834421.py\", line 48, in <module>\n",
      "    explainer = shap.DeepExplainer(model_predict, loader)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py\", line 84, in __init__\n",
      "    self.explainer = TFDeep(model, data, session, learning_phase_flags)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 110, in __init__\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\shap\\explainers\\tf_utils.py\", line 69, in _get_model_inputs\n",
      "AssertionError: <class 'function'> is not currently a supported model type!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\bolou\\miniconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim as optim\n",
    "import shap\n",
    "\n",
    "# Define a simple PyTorch GNN model for graph regression with GCNConv\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, out_feats)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Create a dummy graph dataset\n",
    "# Note: In a real-world scenario, you would load your dataset using PyG's available datasets.\n",
    "x = torch.randn((5, 1))  # Node features (5 nodes, 1 feature per node)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 3, 4], [1, 0, 2, 1, 3, 2, 4, 3]], dtype=torch.long)\n",
    "y = torch.randn((5, 1))  # Target values (regression task)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create a DataLoader\n",
    "loader = DataLoader([data], batch_size=1, shuffle=False)\n",
    "\n",
    "# Create a simple GNN model with GCNConv\n",
    "model = SimpleGNN(in_feats=1, hidden_size=16, out_feats=1)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "# Define a function for the GNN model\n",
    "def model_predict(data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "# Wrap the model_predict function with a shap.DeepExplainer\n",
    "explainer = shap.DeepExplainer(model_predict, loader)\n",
    "\n",
    "# Get Shapley values for a specific data point\n",
    "shap_values = explainer.shap_values(data)\n",
    "\n",
    "# Print Shapley values for each feature\n",
    "print(\"Shapley Values:\", shap_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T06:10:52.811355900Z",
     "start_time": "2023-12-07T06:10:52.749280700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\3908104130.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPlanetoid\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexplain\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExplainer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mGNNExplainer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGCNConv\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 10\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "\n",
    "path = 'feature_importance.png'\n",
    "explanation.visualize_feature_importance(path, top_k=10)\n",
    "print(f\"Feature importance plot has been saved to '{path}'\")\n",
    "\n",
    "path = 'subgraph.pdf'\n",
    "explanation.visualize_graph(path)\n",
    "print(f\"Subgraph visualization plot has been saved to '{path}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T03:38:35.525831900Z",
     "start_time": "2023-12-20T03:38:31.357576100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_15604\\52961935.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch_geometric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mTUDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T11:05:47.687249Z",
     "start_time": "2023-12-20T11:05:47.457318700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2101667002.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\bolou\\AppData\\Local\\Temp\\ipykernel_15604\\2101667002.py\"\u001B[1;36m, line \u001B[1;32m2\u001B[0m\n\u001B[1;33m    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\u001B[0m\n\u001B[1;37m                                                                      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('SGConv', 11, 4), ... 'KendalTau': {0.4176}},\n",
    "    {'gnnConv1': ('GATConv', 1, 1), 'gnnConv2': ('LEConv', 13, 6), ... 'KendalTau': {0}},\n",
    "    {'gnnConv1': ('linear', 2, 2), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4851}},\n",
    "    {'gnnConv1': ('SGConv', 4, 4), 'gnnConv2': ('GENConv', 10, 3), ... 'KendalTau': {0.4852}},\n",
    "    {'gnnConv1': ('LEConv', 6, 6), 'gnnConv2': ('GCNConv', 7, 0), ... 'KendalTau': {0.2727}}\n",
    "]\n",
    "\n",
    "max_kendaltau_entry = max(data, key=lambda x: x['KendalTau'].pop())  # Pop the only element from the set\n",
    "\n",
    "print(\"Entry with the highest KendalTau:\")\n",
    "print(max_kendaltau_entry)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T21:49:04.345165800Z",
     "start_time": "2023-12-21T21:49:04.256696400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
      "Processing...\n",
      "Done!\n",
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parts 128\n",
      "partition Partition(rowptr=tensor([     0,      4,     15,  ..., 163772, 163784, 163788]), col=tensor([  107, 11949, 12057,  ..., 18260, 18276, 18322]), partptr=tensor([    0,   142,   283,   429,   573,   714,   853,   996,  1138,  1283,\n",
      "         1422,  1568,  1707,  1846,  1987,  2134,  2274,  2421,  2564,  2709,\n",
      "         2848,  2995,  3138,  3278,  3420,  3567,  3709,  3856,  3997,  4140,\n",
      "         4285,  4430,  4573,  4720,  4867,  5007,  5149,  5292,  5438,  5577,\n",
      "         5716,  5859,  6001,  6148,  6289,  6429,  6572,  6715,  6858,  7000,\n",
      "         7147,  7294,  7439,  7585,  7732,  7879,  8018,  8160,  8307,  8448,\n",
      "         8591,  8732,  8876,  9022,  9163,  9310,  9449,  9590,  9737,  9876,\n",
      "        10022, 10164, 10310, 10453, 10600, 10744, 10891, 11032, 11179, 11319,\n",
      "        11465, 11612, 11758, 11897, 12042, 12189, 12336, 12477, 12620, 12765,\n",
      "        12904, 13043, 13185, 13328, 13469, 13609, 13748, 13893, 14036, 14182,\n",
      "        14329, 14468, 14607, 14747, 14886, 15027, 15166, 15313, 15460, 15607,\n",
      "        15746, 15887, 16033, 16179, 16325, 16464, 16611, 16750, 16890, 17029,\n",
      "        17168, 17315, 17461, 17608, 17753, 17900, 18047, 18186, 18333]), node_perm=tensor([ 4582, 17980, 11385,  ..., 15945,   108, 17938]), edge_perm=tensor([ 41155,  41156,  41154,  ..., 160456, 160459, 160458]))\n",
      "the size of dataloader is  4\n",
      "Epoch 1: Loss: 2.683008372783661\n",
      "Epoch 2: Loss: 2.617979943752289\n",
      "Epoch 3: Loss: 2.5497769117355347\n",
      "Epoch 4: Loss: 2.483081579208374\n",
      "Epoch 5: Loss: 2.407291829586029\n",
      "Epoch 6: Loss: 2.3410231471061707\n",
      "Epoch 7: Loss: 2.2750619053840637\n",
      "Epoch 8: Loss: 2.211345374584198\n",
      "Epoch 9: Loss: 2.1458704471588135\n",
      "Epoch 10: Loss: 2.088284969329834\n",
      "Epoch 11: Loss: 2.028983235359192\n",
      "Epoch 12: Loss: 1.9800967872142792\n",
      "Epoch 13: Loss: 1.9221903681755066\n",
      "Epoch 14: Loss: 1.8755147457122803\n",
      "Epoch 15: Loss: 1.8241209983825684\n",
      "Epoch 16: Loss: 1.7802663743495941\n",
      "Epoch 17: Loss: 1.729950726032257\n",
      "Epoch 18: Loss: 1.6843460202217102\n",
      "Epoch 19: Loss: 1.6554751992225647\n",
      "Epoch 20: Loss: 1.6117901504039764\n",
      "Epoch 21: Loss: 1.5707997381687164\n",
      "Epoch 22: Loss: 1.531668484210968\n",
      "Epoch 23: Loss: 1.5043930411338806\n",
      "Epoch 24: Loss: 1.4550804793834686\n",
      "Epoch 25: Loss: 1.4284648001194\n",
      "Epoch 26: Loss: 1.4016322493553162\n",
      "Epoch 27: Loss: 1.3668615520000458\n",
      "Epoch 28: Loss: 1.3363281786441803\n",
      "Epoch 29: Loss: 1.3156422972679138\n",
      "Epoch 30: Loss: 1.2831187546253204\n",
      "Epoch 31: Loss: 1.2544763684272766\n",
      "Epoch 32: Loss: 1.2386237382888794\n",
      "Epoch 33: Loss: 1.2115112245082855\n",
      "Epoch 34: Loss: 1.1939606368541718\n",
      "Epoch 35: Loss: 1.1670343577861786\n",
      "Epoch 36: Loss: 1.1511589288711548\n",
      "Epoch 37: Loss: 1.1270447969436646\n",
      "Epoch 38: Loss: 1.1068676710128784\n",
      "Epoch 39: Loss: 1.099424660205841\n",
      "Epoch 40: Loss: 1.0736997723579407\n",
      "Epoch 41: Loss: 1.0578141063451767\n",
      "Epoch 42: Loss: 1.0358800739049911\n",
      "Epoch 43: Loss: 1.017815724015236\n",
      "Epoch 44: Loss: 1.0065890103578568\n",
      "Epoch 45: Loss: 0.9875120371580124\n",
      "Epoch 46: Loss: 0.9834646731615067\n",
      "Epoch 47: Loss: 0.9635068476200104\n",
      "Epoch 48: Loss: 0.9478567987680435\n",
      "Epoch 49: Loss: 0.9377814084291458\n",
      "Epoch 50: Loss: 0.9258021414279938\n",
      "Epoch 51: Loss: 0.9171414822340012\n",
      "Epoch 52: Loss: 0.9022038727998734\n",
      "Epoch 53: Loss: 0.8926699757575989\n",
      "Epoch 54: Loss: 0.8870344310998917\n",
      "Epoch 55: Loss: 0.868302971124649\n",
      "Epoch 56: Loss: 0.854777380824089\n",
      "Epoch 57: Loss: 0.8501127362251282\n",
      "Epoch 58: Loss: 0.8393527716398239\n",
      "Epoch 59: Loss: 0.8257531523704529\n",
      "Epoch 60: Loss: 0.8181094825267792\n",
      "Epoch 61: Loss: 0.8088833540678024\n",
      "Epoch 62: Loss: 0.8016700893640518\n",
      "Epoch 63: Loss: 0.7893040925264359\n",
      "Epoch 64: Loss: 0.7915635257959366\n",
      "Epoch 65: Loss: 0.7736094743013382\n",
      "Epoch 66: Loss: 0.7734422236680984\n",
      "Epoch 67: Loss: 0.7628998011350632\n",
      "Epoch 68: Loss: 0.7571269273757935\n",
      "Epoch 69: Loss: 0.7461705654859543\n",
      "Epoch 70: Loss: 0.7373682707548141\n",
      "Epoch 71: Loss: 0.7380823493003845\n",
      "Epoch 72: Loss: 0.7283009439706802\n",
      "Epoch 73: Loss: 0.7162900567054749\n",
      "Epoch 74: Loss: 0.7147966176271439\n",
      "Epoch 75: Loss: 0.7142381966114044\n",
      "Epoch 76: Loss: 0.7017326653003693\n",
      "Epoch 77: Loss: 0.6985711306333542\n",
      "Epoch 78: Loss: 0.6888579428195953\n",
      "Epoch 79: Loss: 0.688369408249855\n",
      "Epoch 80: Loss: 0.6766800284385681\n",
      "Epoch 81: Loss: 0.6774414926767349\n",
      "Epoch 82: Loss: 0.6717263460159302\n",
      "Epoch 83: Loss: 0.6613534688949585\n",
      "Epoch 84: Loss: 0.6567636132240295\n",
      "Epoch 85: Loss: 0.6547057628631592\n",
      "Epoch 86: Loss: 0.6443182080984116\n",
      "Epoch 87: Loss: 0.6560666412115097\n",
      "Epoch 88: Loss: 0.6455467194318771\n",
      "Epoch 89: Loss: 0.631866917014122\n",
      "Epoch 90: Loss: 0.6247965395450592\n",
      "Epoch 91: Loss: 0.6325894147157669\n",
      "Epoch 92: Loss: 0.6213057041168213\n",
      "Epoch 93: Loss: 0.6184157282114029\n",
      "Epoch 94: Loss: 0.610450267791748\n",
      "Epoch 95: Loss: 0.6127910614013672\n",
      "Epoch 96: Loss: 0.6059034019708633\n",
      "Epoch 97: Loss: 0.6027791500091553\n",
      "Epoch 98: Loss: 0.596782848238945\n",
      "Epoch 99: Loss: 0.589037761092186\n",
      "Epoch 100: Loss: 0.583037331700325\n",
      "Epoch 101: Loss: 0.5833316445350647\n",
      "Epoch 102: Loss: 0.5884296000003815\n",
      "Epoch 103: Loss: 0.5780206471681595\n",
      "Epoch 104: Loss: 0.572365403175354\n",
      "Epoch 105: Loss: 0.5706361085176468\n",
      "Epoch 106: Loss: 0.5687990188598633\n",
      "Epoch 107: Loss: 0.5684967935085297\n",
      "Epoch 108: Loss: 0.5590396076440811\n",
      "Epoch 109: Loss: 0.5566008687019348\n",
      "Epoch 110: Loss: 0.5565603226423264\n",
      "Epoch 111: Loss: 0.559269517660141\n",
      "Epoch 112: Loss: 0.5503171533346176\n",
      "Epoch 113: Loss: 0.5492967814207077\n",
      "Epoch 114: Loss: 0.5405007004737854\n",
      "Epoch 115: Loss: 0.5520153194665909\n",
      "Epoch 116: Loss: 0.5388796180486679\n",
      "Epoch 117: Loss: 0.5457448363304138\n",
      "Epoch 118: Loss: 0.5384775698184967\n",
      "Epoch 119: Loss: 0.5339468643069267\n",
      "Epoch 120: Loss: 0.5296552404761314\n",
      "Epoch 121: Loss: 0.5256039649248123\n",
      "Epoch 122: Loss: 0.5279252156615257\n",
      "Epoch 123: Loss: 0.5214709639549255\n",
      "Epoch 124: Loss: 0.5270162373781204\n",
      "Epoch 125: Loss: 0.5220457464456558\n",
      "Epoch 126: Loss: 0.5125142112374306\n",
      "Epoch 127: Loss: 0.5150868222117424\n",
      "Epoch 128: Loss: 0.5058032795786858\n",
      "Epoch 129: Loss: 0.506624199450016\n",
      "Epoch 130: Loss: 0.5080210566520691\n",
      "Epoch 131: Loss: 0.5069810152053833\n",
      "Epoch 132: Loss: 0.5051781088113785\n",
      "Epoch 133: Loss: 0.5018988773226738\n",
      "Epoch 134: Loss: 0.49811064451932907\n",
      "Epoch 135: Loss: 0.4956810995936394\n",
      "Epoch 136: Loss: 0.49380485713481903\n",
      "Epoch 137: Loss: 0.4904664680361748\n",
      "Epoch 138: Loss: 0.49164681881666183\n",
      "Epoch 139: Loss: 0.49052146822214127\n",
      "Epoch 140: Loss: 0.4912814721465111\n",
      "Epoch 141: Loss: 0.48954949527978897\n",
      "Epoch 142: Loss: 0.4818003848195076\n",
      "Epoch 143: Loss: 0.4777972847223282\n",
      "Epoch 144: Loss: 0.4816071540117264\n",
      "Epoch 145: Loss: 0.4821559861302376\n",
      "Epoch 146: Loss: 0.472384437918663\n",
      "Epoch 147: Loss: 0.46787651628255844\n",
      "Epoch 148: Loss: 0.46734846383333206\n",
      "Epoch 149: Loss: 0.4678941071033478\n",
      "Epoch 150: Loss: 0.4749382883310318\n",
      "Epoch 151: Loss: 0.4756234288215637\n",
      "Epoch 152: Loss: 0.46882083266973495\n",
      "Epoch 153: Loss: 0.4677801877260208\n",
      "Epoch 154: Loss: 0.46584176272153854\n",
      "Epoch 155: Loss: 0.4622360095381737\n",
      "Epoch 156: Loss: 0.4622937962412834\n",
      "Epoch 157: Loss: 0.4628221020102501\n",
      "Epoch 158: Loss: 0.4618065357208252\n",
      "Epoch 159: Loss: 0.4556310921907425\n",
      "Epoch 160: Loss: 0.46281853318214417\n",
      "Epoch 161: Loss: 0.45750464498996735\n",
      "Epoch 162: Loss: 0.4552220404148102\n",
      "Epoch 163: Loss: 0.45765040814876556\n",
      "Epoch 164: Loss: 0.45164867490530014\n",
      "Epoch 165: Loss: 0.45165418833494186\n",
      "Epoch 166: Loss: 0.45204582810401917\n",
      "Epoch 167: Loss: 0.438820980489254\n",
      "Epoch 168: Loss: 0.44352902472019196\n",
      "Epoch 169: Loss: 0.4415833577513695\n",
      "Epoch 170: Loss: 0.44406040012836456\n",
      "Epoch 171: Loss: 0.4476327449083328\n",
      "Epoch 172: Loss: 0.4366539865732193\n",
      "Epoch 173: Loss: 0.4423374980688095\n",
      "Epoch 174: Loss: 0.4375183880329132\n",
      "Epoch 175: Loss: 0.43353528529405594\n",
      "Epoch 176: Loss: 0.43884147703647614\n",
      "Epoch 177: Loss: 0.43254929035902023\n",
      "Epoch 178: Loss: 0.43046244233846664\n",
      "Epoch 179: Loss: 0.4294683337211609\n",
      "Epoch 180: Loss: 0.43316829949617386\n",
      "Epoch 181: Loss: 0.42603693902492523\n",
      "Epoch 182: Loss: 0.42699795216321945\n",
      "Epoch 183: Loss: 0.4283232316374779\n",
      "Epoch 184: Loss: 0.4260401055216789\n",
      "Epoch 185: Loss: 0.4319695010781288\n",
      "Epoch 186: Loss: 0.42136096954345703\n",
      "Epoch 187: Loss: 0.4240872785449028\n",
      "Epoch 188: Loss: 0.41973573714494705\n",
      "Epoch 189: Loss: 0.4216998368501663\n",
      "Epoch 190: Loss: 0.4233466908335686\n",
      "Epoch 191: Loss: 0.41428034007549286\n",
      "Epoch 192: Loss: 0.4123542159795761\n",
      "Epoch 193: Loss: 0.41824574023485184\n",
      "Epoch 194: Loss: 0.40924836695194244\n",
      "Epoch 195: Loss: 0.4127377048134804\n",
      "Epoch 196: Loss: 0.4112326204776764\n",
      "Epoch 197: Loss: 0.41678881645202637\n",
      "Epoch 198: Loss: 0.41235146671533585\n",
      "Epoch 199: Loss: 0.4083682596683502\n",
      "Epoch 200: Loss: 0.41210535168647766\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader,NodeLoader, ClusterLoader, ClusterData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Coauthor\n",
    "from torch_geometric.sampler import BaseSampler\n",
    "\n",
    "Downloader =Coauthor\n",
    "# Assuming dataset is a list of data objects, each representing a graph or a subgraph\n",
    "# dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "dataset=Downloader(root=\"data/test\",name=\"CS\")\n",
    "data=dataset[0]\n",
    "\n",
    "cluster_data = ClusterData(data, num_parts=128)\n",
    "print(\"Num parts\",cluster_data.num_parts)\n",
    "print(\"partition\",cluster_data.partition)\n",
    "loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)\n",
    "print(\"the size of dataloader is \", len(loader))\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index = batch.x, batch.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(dataset.num_node_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00035, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T14:37:34.756642Z",
     "start_time": "2024-02-17T14:35:15.736760400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)  # Get the index of the max log-probability\n",
    "        correct += int(pred.eq(data.y).sum().item())\n",
    "        total += data.x.shape[0]\n",
    "    return correct / total, total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:51.859192800Z",
     "start_time": "2024-02-17T12:20:51.842193900Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_accuracy,total = evaluate(loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:52.311906900Z",
     "start_time": "2024-02-17T12:20:52.261265400Z"
    }
   },
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9523633677991138, 2708)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy, total\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T12:20:52.968052400Z",
     "start_time": "2024-02-17T12:20:52.938969500Z"
    }
   },
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dataset[0]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T14:16:14.785201200Z",
     "start_time": "2024-02-17T14:16:14.770197400Z"
    }
   },
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T04:49:17.346086600Z",
     "start_time": "2024-02-17T04:49:17.317582500Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data2=dataset2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T04:49:18.370197300Z",
     "start_time": "2024-02-17T04:49:18.352195700Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Coauthor, Amazon\n",
    "from torch_geometric.datasets import Planetoid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T11:22:28.748978800Z",
     "start_time": "2024-02-18T11:22:28.389629900Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "CS=Coauthor(root=\"../data/test\",name=\"CS\")[0]\n",
    "cora=Planetoid(root=\"../data/test\",name=\"Cora\")[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T12:48:44.656699700Z",
     "start_time": "2024-02-18T12:47:43.660312100Z"
    }
   },
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object with masks:\n",
      "Data(x=[18333, 6805], edge_index=[2, 163788], y=[18333], train_mask=[18333], val_mask=[18333], test_mask=[18333])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "# Define the sizes for each split\n",
    "train_size = 20\n",
    "val_size = 500\n",
    "test_size = 1000\n",
    "\n",
    "# Create the transform\n",
    "transform = RandomNodeSplit(split=\"random\", num_train_per_class=20, num_val=500,num_test=1000)\n",
    "data = transform(CS)\n",
    "\n",
    "# Output the modified data object\n",
    "print(\"Data object with masks:\")\n",
    "print(data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T12:52:51.055956100Z",
     "start_time": "2024-02-18T12:52:51.025275200Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "f=0\n",
    "for el in data.train_mask:\n",
    "    if el.item()==True:\n",
    "        f+=1\n",
    "print(f)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T12:52:59.960464500Z",
     "start_time": "2024-02-18T12:52:59.826807300Z"
    }
   },
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "mappingproxy({'__module__': 'torch_geometric.loader.cluster',\n              '__doc__': 'Clusters/partitions a graph data object into multiple subgraphs, as\\n    motivated by the `\"Cluster-GCN: An Efficient Algorithm for Training Deep\\n    and Large Graph Convolutional Networks\"\\n    <https://arxiv.org/abs/1905.07953>`_ paper.\\n\\n    .. note::\\n        The underlying METIS algorithm requires undirected graphs as input.\\n\\n    Args:\\n        data (torch_geometric.data.Data): The graph data object.\\n        num_parts (int): The number of partitions.\\n        recursive (bool, optional): If set to :obj:`True`, will use multilevel\\n            recursive bisection instead of multilevel k-way partitioning.\\n            (default: :obj:`False`)\\n        save_dir (str, optional): If set, will save the partitioned data to the\\n            :obj:`save_dir` directory for faster re-use. (default: :obj:`None`)\\n        log (bool, optional): If set to :obj:`False`, will not log any\\n            progress. (default: :obj:`True`)\\n        keep_inter_cluster_edges (bool, optional): If set to :obj:`True`,\\n            will keep inter-cluster edge connections. (default: :obj:`False`)\\n    ',\n              '__init__': <function torch_geometric.loader.cluster.ClusterData.__init__(self, data, num_parts: int, recursive: bool = False, save_dir: Union[str, NoneType] = None, log: bool = True, keep_inter_cluster_edges: bool = False)>,\n              '_metis': <function torch_geometric.loader.cluster.ClusterData._metis(self, edge_index: torch.Tensor, num_nodes: int) -> torch.Tensor>,\n              '_partition': <function torch_geometric.loader.cluster.ClusterData._partition(self, edge_index: torch.Tensor, cluster: torch.Tensor) -> torch_geometric.loader.cluster.Partition>,\n              '_permute_data': <function torch_geometric.loader.cluster.ClusterData._permute_data(self, data: torch_geometric.data.data.Data, partition: torch_geometric.loader.cluster.Partition) -> torch_geometric.data.data.Data>,\n              '__len__': <function torch_geometric.loader.cluster.ClusterData.__len__(self) -> int>,\n              '__getitem__': <function torch_geometric.loader.cluster.ClusterData.__getitem__(self, idx: int) -> torch_geometric.data.data.Data>,\n              '__repr__': <function torch_geometric.loader.cluster.ClusterData.__repr__(self) -> str>,\n              '__parameters__': ()})"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader,ClusterLoader, ClusterData\n",
    "vars(ClusterData)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T15:26:36.525777100Z",
     "start_time": "2024-02-18T15:26:36.499776500Z"
    }
   },
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
